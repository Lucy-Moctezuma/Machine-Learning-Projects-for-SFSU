{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/SFSU-CodeLab-Work-/blob/main/E.%20Coli%20Machine%20Learning%20Project/2_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Learn the basics of how logistic regression model works.\n",
        "- Create functions to implement Logistic Regression to Moradigaravand's dataset.\n",
        "\n",
        "**Logistic Regression** is a classification model that allows us to predict the probability for a binary outcome (2 classes). Typically it is expected that the threshold for logistic regression is 0.5; In our example, this means that above this probability, the model would predict R(Resistant) and below this it will predict S(Susceptible).\n",
        "\n",
        "The equation for Logistic Regression is actually derived from Linear Regression, but instead of Y (Response) we have the natural log of odds:\n",
        "\n",
        "$$ ln(\\frac{P}{1-P}) = \\hat \\beta_0 + \\hat \\beta_jX$$\n",
        "\n",
        "After isolating P, we end up with the equation below:\n",
        "\n",
        "$$P = \\frac{e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}{1+e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}$$\n",
        "\n",
        "- ***P*** is the probability of an outcome. Therefore P is a number between 0(0%) and 1(100%). Our threshold would be 50%, for our example if P < 0.5 our model would predict Susceptibility (S) and if P > 0.5 our model would predict Resistance (R)\n",
        "\n",
        "- $\\hat \\beta_0$ is the intercept term and $\\hat \\beta_j = [\\beta_1 , \\beta_2 , \\beta_3, ... , \\beta_{18293}]$ are the coefficients for each of our features, which the model will try to estimate using our data, there is one coefficient per column feature we are using making 18293 of them in our example.\n",
        "\n",
        "- $X = [Year \\ columns + Gene \\ Absence \\ and \\ Presence \\ Columns + Population \\ Structure \\ Columns]$\n",
        "\n",
        "We will see each of the parts of this equation as we go along in the tutorial, so we can have better picture of these.\n",
        "\n",
        "A full understanding of the math behind the logistic regression model is not necessary. Using a logistic regression model (or any other machine learning model) doesn't require a detailed understanding of the math behind it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n"
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import packages for logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "In here we will be Loading the CSV we created in the previous notebook. This file should contain all out antibiotic drugs (Labels) and all the Years of isolation , Gene Presence/Absence data and the Population Structure data (Features).\n"
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_dfs.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training features, training labels, testing features and testing labels.**\n",
        "\n",
        "The objective of this part will be to create a single dataframe for each antibiotic drug. This means that we want a dataframe with all our features and the label for only one drug. This is because Resistance and Susceptibility are not universal. For example, just because an isolate of *E. coli* is resistant to say AMP (Ampicilin), it doesn't mean that is resistant to AMX (Amoxicilin).\n",
        "\n",
        "Below we can check the list of antibiotics again:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "drug_list = All_Drugs_df.iloc[:,1:13].columns\n",
        "drug_list"
      ],
      "metadata": {
        "id": "U40wWYTyjDtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this we will need to split each of our 12 antibiotic dataframes into 4 different sections:\n",
        "\n",
        "**TRAINING**\n",
        "\n",
        "**a) labels_train:** are the labels (S or R) for a single antibiotic drug that will be used to teach our model how to make predictions.\n",
        "\n",
        "**b) features_train:** are the features that will be used along with the labels_train to teach our model to make predictions. Note that this is actually the X matrix in our logistic equation! They will be used to estimate our $\\beta_0$ and all the $\\beta_j$ values with a process called *Maximum Likelihood*. You can watch the mathematical details of how this is done by watching this [video](https://www.youtube.com/watch?v=BfKanl1aSG0) by Josh Starmer\n",
        "\n",
        "**TESTING**\n",
        "\n",
        "**c) labels_test:** are the labels we will holding out so that we can see at the end if we made accurate predictions.\n",
        "\n",
        "**d) features_test:** are the X values we will plug into our model, once $\\beta_0$ and all the $\\beta_j$ values have already been estimated.\n",
        "\n",
        "- Below we create a function that will be used to separate each of our 12 dataframes into the 4 separate parts described above. Specifically we also specified that 33% of our data to be used as a testing set and thus 67% of our data remains to become our training set. You can choose a different percentage to split them, but know that the majority of our data should be used for training, other splits people try are 30/70 or 20/80 for testing/training respectively.\n",
        "\n",
        "- In addition, the function will save each of these 4 parts into a python Dictionary object. If you are unfamiliar with what a dictionary is in python, feel free to check out this useful [link](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). This way we can organize and access our 4 data chunks for a specific antibiotic drug.\n",
        "\n",
        "**The function created below will create a dataframe and split the data in Training (labels and features) and Testing (labels and features) for each antibiotic.**"
      ],
      "metadata": {
        "id": "pbH90vGNjBGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "  df_list = [All_Drugs_df[[\"Isolate\",drug]], All_Drugs_df.iloc[:,13:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "R61HHXJCX6Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below we can see what our CTZ dictionary contains the 4 chunks explained at the beggining of part 3. We have 1296 training observations and 639 observations were reserved as testing.\n"
      ],
      "metadata": {
        "id": "beftysSPkxs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test() for CTZ example\n",
        "CTZ_Train_test_dic = Split_train_test(\"CTZ\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug CTZ\n",
        "print(\"CTZ\")\n",
        "for k, df in CTZ_Train_test_dic.items():\n",
        "  print(k, df.shape)"
      ],
      "metadata": {
        "id": "E26gr3ULdMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see a count of Susceptible and Resistance strains for the Training and Testing datasets. Notice that there are many more Susceptible than Resistant *E. coli* to CTZ drug. This is considered an \"inbalanced\" dataset. Later on we will discuss how certain metrics may not be as reliable as others when looking at an imbalanced dataset."
      ],
      "metadata": {
        "id": "DIcTBpVQRSya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Training Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_train\"].value_counts()"
      ],
      "metadata": {
        "id": "fcKGS4Ex4AtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Testing Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_test\"].value_counts()"
      ],
      "metadata": {
        "id": "qumUgwdy4vUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "The next part of this project is to add some complexity in our analysis by choosing specifically what sort of features we would like to include. In the original Moradigaravand paper, the authors did the same thing, so we follow their example.\n",
        "Below we have chosen to a specific list of combination of features we are interested in.\n",
        "\n",
        "Recall that we have 3 types of different features we went on detail on our [first notebook](https://colab.research.google.com/drive/1dHGfZGGrSUliH14iVKX-r7pAl_IGiMqm?usp=sharing):\n",
        "- **G**: Gene presence and absence\n",
        "- **S**: Population Structure\n",
        "- **Y**: Years of Isolation\n",
        "\n",
        "We are interested in the following combinations: **G, S, GY, SY, GS, GYS.** This means that for any possible combination of feautures, we will train and test the machine learning models. Then later we can determine which combination of features gives the best results.\n",
        "\n",
        "Below we create a function that will take the features dataframe (train or test) from the dictionary we have created in part 3 and then will create different feature combinations dataframe. For instance, a **\"GS\"** combination implies that we would like to use the Gene presence and absence (G) feature columns and the Years of isolation (Y) columns."
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['G', 'S', 'GY', 'SY', 'GS', 'GYS']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # creating Year column filters for features_df\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Population structure column filters for features_df\n",
        "  pop_str_filter = [col for col in features_df if col.startswith(\"cutoff\")]\n",
        "  pop_struc_feat = features_df[pop_str_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in pop_str_filter and col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'S':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat]\n",
        "    S_feat_df = pd.concat(df_list, axis=1)\n",
        "    S_feat_df = S_feat_df.drop(columns=['Isolate'])\n",
        "    return S_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df\n",
        "\n",
        "  if combo == 'SY':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat, year_feat]\n",
        "    SY_feat_df = pd.concat(df_list, axis=1)\n",
        "    SY_feat_df = SY_feat_df.drop(columns=['Isolate'])\n",
        "    return SY_feat_df\n",
        "\n",
        "  if combo == 'GS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, pop_struc_feat]\n",
        "    GS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GS_feat_df = GS_feat_df.drop(columns=['Isolate'])\n",
        "    return GS_feat_df\n",
        "\n",
        "  if combo == 'GYS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat, pop_struc_feat, ]\n",
        "    GYS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GYS_feat_df = GYS_feat_df.drop(columns=['Isolate'])\n",
        "    return GYS_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "CTZ_GYS_train_df = combo_feat(CTZ_Train_test_dic['features_train'],\"CTZ\",\"GYS\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GYS\" for drug \"CTZ\" for training data\n",
        "CTZ_GYS_train_df.columns"
      ],
      "metadata": {
        "id": "sqvvdtJOu_RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Logistic regression model and training it per feature combination**\n",
        "\n",
        "The next step involves creating a function that will actually creat our Logistic Regression model and train it on our desired combination of training features and the labels for the drug we choose. While this function seems fairly straight forward, there is a lot of calculations happening in the backrgound when we call the LG.fit() function which trains our model. We will only take a small peak at what's going on in the background. Running this function will also take more time than the other functions we have run until now.\n"
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Logistic regression model function\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def run_LG(feat_train_df, lab_train, drug, combo):\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  LG = LogisticRegression(random_state = 42, solver= 'lbfgs', C=1.0, max_iter=500, class_weight='balanced')\n",
        "  LG = LG.fit(feat_train_df, lab_train)\n",
        "  return LG"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_LG() for specific drug feature combination dataframe\n",
        "LG_CTZ_GYS_model = run_LG(CTZ_GYS_train_df, CTZ_Train_test_dic['labels_train'],\"CTZ\",\"GYS\")\n",
        "LG_CTZ_GYS_model"
      ],
      "metadata": {
        "id": "_HYfVIgUvEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see that our model contains several coefficients. We will printout first the intercept ($\\hat{\\beta}_{0}$), then a list of the coefficients that correspond to each column feature ($\\hat{\\beta}_{j}$) and finally we can get a print out of the total number of coefficients in our model, these should be the same number as all the features we used in this example."
      ],
      "metadata": {
        "id": "ynlnJ_21mH7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the beta_0 or intercept value of our model\n",
        "print(\"Intercept:\",LG_CTZ_GYS_model.intercept_[0])\n",
        "\n",
        "# printing all the beta_j's or coefficients of our logistic regression model\n",
        "print(\"All beta_j values:\", LG_CTZ_GYS_model.coef_[0])\n",
        "\n",
        "# printing the number of all the beta_j values\n",
        "print(\"Number of beta_j values: \", len(LG_CTZ_GYS_model.coef_[0]))"
      ],
      "metadata": {
        "id": "t99qNo2bvF8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Logistic regression model**\n",
        "\n",
        "Now that our model has been trained and all $\\beta$ values have been estimated, we are ready to make predictions! We will use the features of our testing data which we separated when we made our antibiotic drug dictionary.\n",
        "\n",
        "Below we create another function where we predict labels using the actual model and the \"features_test\" chunk.\n"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(LG_combo_Model, features_test):\n",
        "  labels_pred = LG_combo_Model.predict(features_test)\n",
        "  return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will use the function **combo_feat()** to split the testing dataset"
      ],
      "metadata": {
        "id": "wwYUz_D4Un57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "CTZ_GYS_test_df = combo_feat(CTZ_Train_test_dic['features_test'],\"CTZ\",\"GYS\")"
      ],
      "metadata": {
        "id": "xifjI6iHvK66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The we will implement the function **predict()** below:"
      ],
      "metadata": {
        "id": "kYZfi84wmweT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the predict() function using the feature combination \"GYS\"\n",
        "CTZ_GYS_labels_pred = predict(LG_CTZ_GYS_model,CTZ_GYS_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTZ_GYS_labels_pred, return_counts=True))\n"
      ],
      "metadata": {
        "id": "EE1fy63XvI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in the last output that index order for **R (Resistance is 0)** and for **S (Susceptible is 1)**, meaning that we predicted 120 Resistant and 519 Susceptible samples."
      ],
      "metadata": {
        "id": "fi1DXXxfwUzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the predictions for the first 30 isolates\n",
        "print(\"Labels predicted for first 10 test isolates: \", CTZ_GYS_labels_pred[:30])\n",
        "\n",
        "# and the actual labels for the first 230 isolates â€“ do they match?\n",
        "print(\"Labels predicted for first 10 test isolates: \", np.array(CTZ_Train_test_dic['labels_test'][:30]))\n"
      ],
      "metadata": {
        "id": "_QGUYMhg5Gy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "There are different ways we can evaluate our model. A **confusion matrix** is a plot showing a prediction summary for our model. It allows us to see how many predictions were correct and incorrect. There are also different metrics we can calculate using this graph. For this tutorial we will focus on three of them: **Accuracy**, **Recall** and **Precision**.\n",
        "\n",
        "- **Accuracy:** is the total number of correct classifications over the total amount of predictions made.\n",
        "\n",
        "- **Recall:** is the number of correct classifications made for a particular class over all predictions of that class.\n",
        "\n",
        "- **Presicion:** is the number of classifications made for a particular class over the actual number of strains for that class.\n",
        "\n",
        "Recall and Precision can each be calculated for resistance and for susceptibility.\n",
        "\n",
        "When we have two classes, a 2 by 2 confusion matrix contains:\n",
        "\n",
        "- **True Positives (TP)** = Resistant strains correctly classified as resistant (R) = 48\n",
        "- **True Negatives (TN)** = Susceptible strains correctly classified as susceptible (S) = 479\n",
        "- **False Positives (FP)** = Susceptoble strains incorrectly classified as resistant (R) = 72\n",
        "- **False Negatives (FN)** = Resistant strains incorrectly classified as susceptible (S) = 40"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(LG_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=LG_combo_model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LG_combo_model.classes_)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "  return [report['accuracy'], report['R']['recall'], report['S']['recall'], report['R']['precision'], report['S']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we implement our function we can show a manual way in which these metrics are calculated, first for the overall *Accuracy* and then *Recall* and *Precision* for only the Resistant strains (R) as an example:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|<font size=3>Manual Calculation|\n",
        "|--|:-:|:-:|:--|\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|<font size=3>$\\frac{48 + 479}{639} = 0.8247$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|$\\frac{48}{48 + 40} = 0.545$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|<font size=3>$\\frac{48}{48 + 72} = 0.4$|\n",
        "\n",
        "\n",
        "**NOTE:** In this tutorial we only work with 2 classes (R and S), thus the abbreviations (TP, TN, FP and FN) apply to our confusion matrix, however for situations with more than 2 classes, refer to the general formula column."
      ],
      "metadata": {
        "id": "b0uqIS4V0Il4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(LG_CTZ_GYS_model, CTZ_Train_test_dic['labels_test'],CTZ_GYS_labels_pred)"
      ],
      "metadata": {
        "id": "AM3fq7QqvNqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in this example, accuracy is 82%. That sounds quite good. But nearly half of the resistant cases are misclassified as susceptible. So, if the main goal here is to detect resistant strains, this particular model does not perform well at all."
      ],
      "metadata": {
        "id": "YNSVkF2G9bW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) Use all functions and evaluate every drug in every feature combination!**\n"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all feature combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**\n",
        "\n",
        "The code below will take longer to run because we will train our logistic regression using all feature combinations for each of the antibiotic drugs. it will then be stored in a python dictionary.\n",
        "\n",
        "**NOTE:** Python Dictionaries, are common data structures within the python language that are very efficient at storing and retrieving information. It consists in key-value pairs, in our case each key is: drug_combo and the value is paired with are the metrics results."
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use all our functions this time and save our report into a single data structure\n",
        "LG_model_metrics = {}\n",
        "\n",
        "for drug in drug_list:\n",
        "  print(drug)\n",
        "  # splits each drug df into a dictionary with testing and training data\n",
        "  Test_Train_dic = Split_train_test(drug)\n",
        "  for combo in combo_list:\n",
        "    # Training each drug and combo features\n",
        "    labels_train = Test_Train_dic[\"labels_train\"]\n",
        "    # create corresponding feature_df for training\n",
        "    features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "    # runs logistic regression model using the corresponding training feature_df\n",
        "    LG_combo_model = run_LG(features_train, labels_train, drug, combo)\n",
        "    # create corresponding feature_df for testing\n",
        "    features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "    # generate predictions based on the feature combination tested\n",
        "    labels_pred = predict(LG_combo_model, features_test)\n",
        "    # loading the actual labels\n",
        "    labels_test = Test_Train_dic[\"labels_test\"]\n",
        "    # Evaluating our model\n",
        "    report = evaluate(LG_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "    # Saving the results into a dictionary\n",
        "    LG_model_metrics[drug+\"_\"+combo] = report\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "glSAeIj2DrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Store the metrics report for all drugs and features combinations as a csv file**\n",
        "\n",
        "Using the dictionary we created above to create a final dataframe for all our results and store them as a csv file we can access afterwards."
      ],
      "metadata": {
        "id": "m8CQMKARZ1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dictionary into a dataframe\n",
        "LG_metrics = pd.DataFrame.from_dict(LG_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "LG_metrics = LG_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "# saving our metric results into a CSV file\n",
        "LG_metrics.to_csv(filepath+\"LG_metrics_df.csv\", index= False)\n",
        "LG_metrics"
      ],
      "metadata": {
        "id": "VmNdWS65fhRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **c) Create a bar graph showing metrics for all drugs when using all three types of features (GYS)**\n",
        "Below we will create a bargraph to visualize metrics for every antibiotic that uses the feature combination GYS (Gene Presence and Absence, Year of Isolate collection and Population Structure). In the graph below we are comparing Accuracy and Recall scores for both classes. Notice for instance the antibiotic **CTZ**, when we only pay attention to the Accuracy, we would conclude that our model is doing fairly well, however, when examine recall we can clearly see that the Recall for Resistance is noticeable lower than for Susceptibility. Therefore, for imbalanced classes, Accuracy may not always be the best metric to use, or at least not without also looking at other metrics."
      ],
      "metadata": {
        "id": "2-6NzvrsbZaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below serves to create a new directory to store all our Machine Learning plots and figures created. If you run the code more than once it will give an error, that you can ignore this message, since the directory has already been created."
      ],
      "metadata": {
        "id": "PXsvRlZk4fxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# makes a directory for all your plot images\n",
        "os.mkdir('/content/drive/My Drive/EColi_ML_Plots')"
      ],
      "metadata": {
        "id": "z0ZUo3fP4pUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering for all the rows that contain GYS features\n",
        "GYS_filter = [drug_combo for drug_combo in LG_metrics['Drug_combo'] if drug_combo.endswith(\"GYS\")]\n",
        "GYS_df = LG_metrics.loc[LG_metrics[\"Drug_combo\"].isin(GYS_filter)]\n",
        "\n",
        "# Figure Size\n",
        "fig = plt.figure(figsize =(20, 8))\n",
        "\n",
        "# Adding title\n",
        "plt.title('Accuracy, R_recall, S_recall', fontsize = 12)\n",
        "\n",
        "# Variables to be plotted\n",
        "x = np.arange(len(GYS_df[\"Drug_combo\"]))\n",
        "acc = list(GYS_df[\"Accuracy\"])\n",
        "R_rec = list(GYS_df[\"R_recall\"])\n",
        "S_rec = list(GYS_df[\"S_recall\"])\n",
        "\n",
        "# Plotting barcharts\n",
        "acc_bar=plt.bar(x-0.25, height= acc, width=0.25, color=\"grey\", edgecolor=\"gray\")\n",
        "rrec_bar=plt.bar(x, height= R_rec, width=0.25, color=\"plum\", edgecolor=\"gray\")\n",
        "srec_bar=plt.bar(x+0.25, height= S_rec, width=0.25, color=\"lavenderblush\", edgecolor=\"gray\")\n",
        "\n",
        "plt.xticks([r for r in range(len(GYS_df[\"Drug_combo\"]))],\n",
        "        GYS_df[\"Drug_combo\"], fontsize = 12)\n",
        "\n",
        "#legend\n",
        "fig.legend([acc_bar,rrec_bar,srec_bar],[\"Accuracy\", \"R_recall\", \"S_recall\"], bbox_to_anchor=(0.4,-0.35, 0.04, 0.4), fontsize=12)\n",
        "\n",
        "# Saving bargraph into our new directory\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/GYS_LG_Accuracy_and_Recall_Scores.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dQs9pOMYvUg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks for making it this far! We have accomplished the objectives of this notebook. We learned the basics of how logistic regression works and learned how to make functions in order to run this model using Moragadivand's *E. coli* dataset. Now we will move on to our next ML model, [Random Forest](https://colab.research.google.com/drive/13XuIu7AsR6HTEerT9lyoLn3lWQjUvXg-?usp=sharing) and learn a bit more about tree-based models in general."
      ],
      "metadata": {
        "id": "xngjGG3BF47x"
      }
    }
  ]
}