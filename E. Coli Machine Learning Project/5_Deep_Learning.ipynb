{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/SFSU-CodeLab-Work-/blob/main/E.%20Coli%20Machine%20Learning%20Project/5_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning (Neural Network)**\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Introducing the importance of feature scaling and basic concepts behind Neural Networks used in the context of tabular data.\n",
        "- Create functions to implement Neural Networks into Moragadivand's dataset.\n",
        "\n",
        "**Deep Learning or Neural Networks**  is a method within Machine Learning (ML) in which the computer learns to accomplish a task through trial and error by analyzing training samples. Neural networks are loosely inspired by how biological neurons are connected and signal each other. It's important to know that there are many kinds of neural networks that can be used for different kinds of data (images, sounds, etc) and there are specific names for different Neural network architectures (Convolutional Neural Networks, Recurrent Neural Networks, etc.). We will however focus only with basic Neural Network for tabular data (i.e. dataframes).\n",
        "\n",
        "### **Parts of a Neural Network**\n",
        "\n",
        "All neural networks despite of their function and kind of data they deal with have in essence these following parts.\n",
        "\n",
        "![neuralnet.png](https://drive.google.com/uc?export=view&id=1df7Dq1LS9QTFOdLuvOWb1Et6IiMIq4FK)\n",
        "\n",
        "**1) Input layer:** is the layer that we use to feed our initial data. These can be data tables, text, images, etc. In our case we would be working with a data table, and this layer will contain the same amount of nodes as there are feature columns in our dataset.\n",
        "\n",
        "**2) Hidden layers:** are the ones that will further process the information they receive from the input layer. In the example above we have 2 hidden layers but the amount of layers can vary depending on the task.\n",
        "\n",
        "**3) Output layer:** is the final layer where we get our predictions, because we are dealing with classification of just 2 classes we will have 2 nodes in this layer.\n",
        "\n",
        "**4) Nodes:** are the the components of each layer and it represents a center where computation and mathematical equations determine what information is passed to the next layer. Nodes are connected to the following layers differently.\n",
        "\n",
        "**5) Weights:** are values that are meant to show the strenght of the relationship between each node. The general idea is that a neural network starts with a random set of weights and then during training, the weights get updated in a trial and error fashion until it finds the best combination of weights that will yield the highest performing model.\n",
        "**Notice that in the image above, every black arrow has its own weight**\n"
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n",
        "For Neural Networks we will introduce some new python packages: **Keras** and **Tensorflow**. Both of these are widely used within the Python community to construct Neural Networks."
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import packages for Neural Networks model\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import keras as keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SEED SETTING**\n",
        "\n",
        "There is a lot of randomnes that contribute to Neural Networks results. Nevertheless, a good Neural Network Model should not depend on the seed but the data used and the architechture, given this, metrics should not be extremely affected by seed setting. Therefore this randomness is actually desired and not a problem.\n",
        "\n",
        "However, for the sake of this learning tutorial and for reproducibility of these results we will be fixing **seed_value: 42** for all the background computations. Below we list the different sources of randomnes:\n",
        "\n",
        "- Within Environment\n",
        "- Within Python language\n",
        "- Within specific packages (numpy, tensorflow)\n",
        "- When choosing splits for training and testing data\n",
        "- Within Learning Algorythm:\n",
        "  - Neural network sets random weights at the beggining of training\n",
        "  - Some special layers such as Dropout layers introduce randomness\n",
        "\n",
        "The code below will **seed_value: 42** globally for the environment, the **python** language and for 2 packages (**numpy and tensorflow**). We will also quickly indicate where we have set other seeds. Everytime a seed is set you will see `seed_value` within the code of this notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "RS3oD-bjJFY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a seed value\n",
        "seed_value= 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "sd-BLHih2XpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "\n",
        "As in all previous ML model Notebooks, We will load our original dataframe, which contains all antibiotic drug labels and all features (GYS) and then we will be creating a dataframe for each antibiotic using different functions."
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Loading CSV created from previous notebook**\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "QNuEJfKcVSLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_dfs.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Changing \"R\" to 0 and \"S\" to 1  for Deep Learning Model**\n",
        "Neural Networks work using numeric values, therefore we will be converting all our target labels into floats. The last line of code shows the recoded version of all the labels for the antibiotic drugs."
      ],
      "metadata": {
        "id": "Gnvbqq1Eduja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a list of antibiotic names\n",
        "drug_list = All_Drugs_df.iloc[:,1:13].columns\n",
        "\n",
        "# converts all S values into 1 for each antibiotic\n",
        "for drug in drug_list:\n",
        "  All_Drugs_df.loc[All_Drugs_df[drug] == \"S\", drug] = 1.0\n",
        "\n",
        "# converts all R values into 0 for each antibiotic\n",
        "for drug in drug_list:\n",
        "  All_Drugs_df.loc[All_Drugs_df[drug] == \"R\", drug] = 0.0\n",
        "\n",
        "# Checking at how S and R classes were recoded\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "lLeCU30hD0Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training (features and labels) and Testing (features and labels)**\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) Creating Testing and Training datasets for each antibiotic drug**\n",
        "\n",
        " *Seed value was used in the train_test_split() function, to split the data consistentently, that is, the observations chosen to be part of the training chunk will be consistent regardless of how many times the code is ran.*\n",
        "\n"
      ],
      "metadata": {
        "id": "B2BblFQkce6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "  df_list = [All_Drugs_df[[\"Isolate\",drug]], All_Drugs_df.iloc[:,13:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=seed_value, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "iP4HEW4qBO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test() for AMC example\n",
        "AMX_Train_test_dic = Split_train_test(\"AMX\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug AMC\n",
        "print(\"AMX\")\n",
        "for k, df in AMX_Train_test_dic.items():\n",
        "  print(k, df.shape)\n",
        "  # counting how many of the labels have susceptible versus resistant ones\n",
        "  if k.startswith(\"labels\"):\n",
        "    print(df.value_counts())"
      ],
      "metadata": {
        "id": "43qPwvrrqCaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that our target values have been recoded. **0.0 is Resistance (R)** and **1.0 is Susceptible (S)**. And in our implementation we used the antibiotic AMX. The total number of training observations is 732."
      ],
      "metadata": {
        "id": "imprnOtpedzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "Similar to previous notebooks, we have created a function that helps us choose independently what group of features we would like to train with (Gene Absence Presence or Absence (**G**), Year of Isolation (**Y**) and/or Population Structure (**S**). But for Neural Networks, we have added a subtle but important difference between this function and the prior ones created. In here we specifically ask our features to be **Scaled**.\n",
        "\n",
        "**Scaling:** is a technique where we try to change the numeric values in all columns so that they share the same scale. Scaling is particularly useful for Neural Networks as it helps the model find the minima of the cost function quicker. There are different ways to rescale data, the two main ways are: [Normalization and Standarization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/#Why_Should_We_Use_Feature_Scaling?).\n"
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['G', 'S', 'GY', 'SY', 'GS', 'GYS']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # creating Year column filters for features_df\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Population structure column filters for features_df\n",
        "  pop_str_filter = [col for col in features_df if col.startswith(\"cutoff\")]\n",
        "  pop_struc_feat = features_df[pop_str_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in pop_str_filter and col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = scale(G_feat_df.drop(columns=['Isolate']))\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'S':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat]\n",
        "    S_feat_df = pd.concat(df_list, axis=1)\n",
        "    S_feat_df = scale(S_feat_df.drop(columns=['Isolate']))\n",
        "    return S_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = scale(GY_feat_df.drop(columns=['Isolate']))\n",
        "    return GY_feat_df\n",
        "\n",
        "  if combo == 'SY':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat, year_feat]\n",
        "    SY_feat_df = pd.concat(df_list, axis=1)\n",
        "    SY_feat_df = scale(SY_feat_df.drop(columns=['Isolate']))\n",
        "    return SY_feat_df\n",
        "\n",
        "  if combo == 'GS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, pop_struc_feat]\n",
        "    GS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GS_feat_df = scale(GS_feat_df.drop(columns=['Isolate']))\n",
        "    return GS_feat_df\n",
        "\n",
        "  if combo == 'GYS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat, pop_struc_feat]\n",
        "    GYS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GYS_feat_df = scale(GYS_feat_df.drop(columns=['Isolate']))\n",
        "    return GYS_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the function we created, we scale the features using **scale()**, which actually performs an **Standarization** (where values are mean centered and distance from mean is measured in standard deviations). Without scaling values, the Neural Network could get biased towards features that have a higher order of magnitude.\n",
        "\n",
        "Below we can see an example of the difference between raw values and standarized values for two columns. Notice that in raw residuals the standard deviations are not the same. Without scaling its hard to compare which variable of the two might be more important as their ranges differ.\n",
        "\n"
      ],
      "metadata": {
        "id": "_KHssYisntFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features before scaling: Raw values\n",
        "AMX_Train_test_dic['features_train'][['cutoff_25459','cutoff_2']].describe()"
      ],
      "metadata": {
        "id": "yLFR83R2gt_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have the scaled features and we can notice the following:\n",
        "- All feature descriptives get smaller\n",
        "- The means and ranges are still different like in our unscaled data\n",
        "- The standard deviations are the same now.\n",
        "\n",
        "Now we can compare the values based on the number of standard deviations away from their mean. We are able to tell within the same observation and for both variables whether they represent a typical value (sd closer to mean) or an extreme value (sd further from mean). Now our model has an easier way to tell which of the two features might be more relevant.\n",
        "\n",
        "It's important to know that feature scaling may not always make a difference, it really depends on the type of Machine Learning algorithm you choose. For example, we did not do it for previous notebooks or tree based methods, but we did it in here. You can read more about it in this summary [article](https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048#:~:text=Feature%20scaling%20is%20the%20process,need%20to%20perform%20feature%20scaling.)"
      ],
      "metadata": {
        "id": "lTbGyW28oLRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features after scaling: standarization\n",
        "feat_scaled = scale(AMX_Train_test_dic['features_train'][['cutoff_25459','cutoff_2']])\n",
        "feat_scaled = pd.DataFrame(feat_scaled, columns=[['cutoff_25459_(scaled)','cutoff_2_(scaled)']])\n",
        "feat_scaled.describe()"
      ],
      "metadata": {
        "id": "AYnWgwAjhV8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to implement our function in a complete training dataset. Instead of having just 2 columns being scaled, it will do it for all the feature columns in the G and S datasets for the antibiotic \"AMX\"."
      ],
      "metadata": {
        "id": "NAHiY5YNuS1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "AMX_GS_train_array = combo_feat(AMX_Train_test_dic['features_train'],\"AMX\",\"GS\")\n",
        "\n",
        "# Each list within the array represents a row\n",
        "AMX_GS_train_array"
      ],
      "metadata": {
        "id": "z6f4cke-qHxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Deep Learning model and training it per feature combination**\n",
        "Neural Networks contain many hyperparameters we can tune, the input layer in this case doesn't need to be set up as it is determined by the amount of features we are using to train. But we will be setting the hidden layers and the final output layer.:\n",
        "- The number of nodes for the hidden first layer.\n",
        "- The number of hidden layers we want after the first one.\n",
        "- The number of nodes for the subsequent layers\n",
        "- The dropout rate for hidden layers\n",
        "- The learning rate\n"
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nodes on first hidden layer\n",
        "firstlayer = 200\n",
        "\n",
        "# Number of hidden layers\n",
        "numblayer = 2\n",
        "\n",
        "# Number of nodes on each hidden layer\n",
        "interlayer = 100\n",
        "\n",
        "# Dropout rate\n",
        "dropout = 0.8  # default was 0.8\n",
        "\n",
        "# Learning rate\n",
        "lr= 0.001"
      ],
      "metadata": {
        "id": "MET6tCQ4O_Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After setting some of our Neural Network Paramenters, we can create our function. The other parameters and the architecture of the model is based on how Moragadivand's had it set up for the paper (number of layers and nodes, other parameter settings, etc).\n",
        "\n",
        "The general structure of how each layer is created is shown below in the pseudo code, we will also briefly define some vocabulary used in our specific code:\n",
        "\n",
        "<font color=\"grey\">`Model.add(Layertype(#_output_weights, activation= 'specific function', input_shape=#_input_weights)`\n",
        "\n",
        "For Neural Network *Architechture*:\n",
        "- **Dense:** This is a kind of layer, where each node from a dense layer receives conexions from all the previous nodes, for this reason they are often called **\"fully connected\"**.\n",
        "\n",
        "- **Dropout:** This is a special type of layer with the sole function to block a percentage of weights that pass from one layer to the next. A Dropout rate assigns a percentage of randomly selected neurons in a layer to become inactive, meaning their contribution in the forward pass is not taken into account and therefore the weights in the backward pass towards these dropout neurons do not get updated. This is considered to be a **Regularization method**, meaning it is done so that our model doesn't follow the training data too closely and is able to generalize better for unseen data.\n",
        "\n",
        "- **Activation function:** is a parameter we can pass for some types of layers, it is essentially a function that computes the output for a layer. For example, for Dense layers we have [\"Relu\"](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), whereas for dropout layers we dont have one, as they don't need to compute an output, they just block outputs from the layer before from going to the next layer, by converting some of these to 0.\n",
        "***\n",
        "For Neural Network *Training behavior*:\n",
        "\n",
        "<font color=\"grey\">`EarlyStopping(patience=# training epochs)`\n",
        "\n",
        "- **Early Stopping:** is a parameter that essentially stops the training based on the maximum number of epochs accepted before stopping the training if no improvement is seen.\n",
        "\n",
        "<font color=\"grey\">`Model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr) ,loss= 'binary_crossentropy', metrics=['accuracy'])`\n",
        "\n",
        "- **Learning rate**: is a value between 0 and 1, that determines basically how fast our model will try to learn. The higher the learning rate, the less number of epochs (training cycles) is required. This is the most important parameter to tune in a Neural Network. Too large and it will converge too quickly and provide a suboptimal result, too small and it will get stuck.\n",
        "\n",
        "- **Loss function**: there are many loss functions we can use depending on the type of data and task. A loss function is essentially a method used to evaluate how well our algorithm models our training data. Our objective is to minimize it since its essentially our errors. In our case we use:  *binary_crossentropy* (a mathematical function that measures the difference between predicted probabilities and actual labels in classification task with only 2 classes)\n",
        "\n",
        "- **Metrics**: this is just a measurement we can look at in order to judge the performance of the model while training, and as our loss function is being optimized.\n",
        "***\n",
        "*Seed_values were set here for the initial weights prior to training and for every Dropout layer in the Neural Network*"
      ],
      "metadata": {
        "id": "b-4ZZyH2ae7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Deep Learning model function\n",
        "def run_DL(feat_train_df, lab_train, drug, combo, view_training = True):\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "\n",
        "  # Reweighting classes due to imbalanced dataset\n",
        "  class_labels = np.unique(lab_train)\n",
        "  reweight = compute_class_weight(class_weight='balanced', classes=class_labels, y=lab_train)\n",
        "\n",
        "  # Constructing Deep Learning model\n",
        "\n",
        "  # Choose the type of Neural Network you want create\n",
        "  DL = Sequential()\n",
        "\n",
        "  # Adding the first hidden layer.\n",
        "  DL.add(Dense(int(firstlayer),activation='relu', input_shape=(feat_train_df.shape[1],)))\n",
        "\n",
        "  # Adding the dropout layer for the first layer.\n",
        "  DL.add(Dropout(dropout, input_shape=(feat_train_df.shape[1],), seed=seed_value))\n",
        "\n",
        "  # Create more hidden layers\n",
        "  for i in range(1,int(numblayer)):\n",
        "      DL.add(Dense(int(interlayer),activation='relu'))\n",
        "\n",
        "      # For each of the hidden layers also create dropout\n",
        "      DL.add(Dropout(dropout, seed=seed_value))\n",
        "\n",
        "  # Create the output layer that consist only on 2 nodes (we have 2 classes)\n",
        "  DL.add(Dense(2, activation = 'softmax', kernel_initializer=keras.initializers.glorot_uniform(seed=seed_value)))\n",
        "\n",
        "  # Additional parameters for training (Early stopping)\n",
        "  early_stopping_monitor= EarlyStopping(patience=50)\n",
        "\n",
        "  # Compiling model created\n",
        "  DL.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr) ,loss= 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Training with the neural network created\n",
        "  DL_history = DL.fit(feat_train_df, to_categorical(lab_train), validation_split=0.2, callbacks= [early_stopping_monitor],epochs=10, batch_size=128, class_weight=dict(enumerate(reweight)))\n",
        "\n",
        "  # visualizing each training\n",
        "  if view_training == True:\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle('Training History')\n",
        "    print(DL_history.history.keys())\n",
        "    ax1.plot(DL_history.history['accuracy'])\n",
        "    ax1.plot(DL_history.history['val_accuracy'])\n",
        "    ax1.set_title('model accuracy')\n",
        "    ax1.set(ylabel='accuracy', xlabel='epochs')\n",
        "\n",
        "    ax2.plot(DL_history.history['loss'])\n",
        "    ax2.plot(DL_history.history['val_loss'])\n",
        "    ax2.set_title('model loss')\n",
        "    ax2.set(ylabel='loss', xlabel='epochs')\n",
        "    ax2.legend(['train', 'validation'], loc='best')\n",
        "    fig.tight_layout()\n",
        "    filepath='/content/drive/My Drive/EColi_ML_Plots/'\n",
        "    plt.savefig(filepath + 'DL_' + drug + '_' + combo + '_' + 'training_history.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "  return DL\n"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=\"grey\">`Model.fit(features, to_categorical(labels), validation_split = percent of training data used for validation, callbacks= [early_stopping_monitor],epochs=# of training cycles, batch_size= # observations fed at a time))`\n",
        "\n",
        "A Neural Network, is trained in 2 general stages:\n",
        "\n",
        "**1) Feedforward**: First the observations start on the input layer go through all the hidden layers and then it produces an initial prediction in the output layer.\n",
        "\n",
        "**2) Backpropagation**: An error is calculated between our actual training labels and the output we got from the Feedforward process. It then adjusts the weights from output layer back to input layer in order to minimize the errors.\n",
        "***\n",
        "**Epoch:** Is one cycle of training where the entire dataset goes through a Feedforward process and a Backpropagation one. We have set a total of 10 epochs.\n",
        "\n",
        "**Batch:** Because feeding the entire dataset might be computationally expensive, it is a common practice to feed our neural network data in batches, we have chosen a batch size of 128.\n",
        "\n",
        "**Validation Data:** Is a chunk of the training data that is reserved to check if our model is generalizing well to all the training data. This is different from the testing data, because it is being used during model training.\n",
        "\n",
        "Our total number of observations is 732, and our batch size is 128. Therefore, in one epoch we will have 732/128, so about 5 batches per epoch. Below we will implement our function for the antibiotic AMX and using only the features (G and S)."
      ],
      "metadata": {
        "id": "MDG1-shpYU3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_DL() for specific drug feature combination dataframe\n",
        "DL_AMX_GS_model = run_DL(AMX_GS_train_array, AMX_Train_test_dic['labels_train'],\"AMX\",\"GS\")"
      ],
      "metadata": {
        "id": "kJgB0ELDqLVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the graph above we can see that training and validation data have an upwards direction, meaning that we can see that accuracies for both training and validation are going up, meaning there is learning happening.\n",
        "- We notice that the **training set** has a lower overall accuracy and a higher loss than the **validation set**, this is not usually the case, we would normally observed the reversed pattern. Because the validation is supposed to simulate unseen data it should have lower accuracies and higher loss, but because **Dropout layer** is implemented, regularization happens and this generalizability makes the validation metrics better. You can test this by lowering the dropout rates in a new session of this notebook, in doing so the lines will beggin to overlap and we will start to see more of a reversed pattern. The current dropout rate is at 0.8.  \n",
        "\n",
        "In the code below we can graph our Neural Network model architechture:\n"
      ],
      "metadata": {
        "id": "q3SAcPzlM5gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the schematics of our model\n",
        "plot_model(DL_AMX_GS_model, to_file='/content/drive/MyDrive/EColi_ML_Plots/DL_AMX_GS_model_Architechture.jpg', show_shapes=True, show_layer_activations=True, rankdir='LR')"
      ],
      "metadata": {
        "id": "HRqUdyhfNGcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in the figure that the Input layers input contains 18269 nodes, this is because each node pertains to one feature. Remember that this model was trained with only G and S dataset features. Our input layer nodes will change depending on out training data.\n",
        "\n",
        "For each layer we can see the following information displayed:\n",
        "- The name of the layer (example: dense_input)\n",
        "- The type of neurons the layer contains (example: Dense, Droput)\n",
        "- The activation function the layer uses (example: softmax, relu)\n",
        "- The amount of connections for the nodes it receives (input) - The amount of connections leaving the node (output). For dropout layers, we see that input and output shape are the same. However, when a weight gets filtered here, the amount passed is 0. Also, which neurons' weights become 0 changes every cycle of training, which is why everytime we run the code it will give you slightly different results.\n"
      ],
      "metadata": {
        "id": "QgSQCNDH_KlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Deep Learning model**\n",
        "\n",
        "After training our Neural network we are now ready to make predictions."
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(DL_combo_Model, features_test):\n",
        "  labels_pred = DL_combo_Model.predict(features_test)\n",
        "  labels_pred = np.argmax(labels_pred, axis=1)\n",
        "  return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use our **combo_feat()** function to choose the appropiate data feature combination for Testing. In the example below, we use the combination of features G and S."
      ],
      "metadata": {
        "id": "HwNlTWVIHJ__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "AMX_GS_test_array = combo_feat(AMX_Train_test_dic['features_test'],\"AMX\",\"GS\")\n",
        "\n",
        "# Each list within the array represents a row\n",
        "print(AMX_GS_test_array)\n",
        "print(\"Number of rows: \",len(AMX_GS_test_array))"
      ],
      "metadata": {
        "id": "cbzJt4FaqTIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will implement our **predict()** function and checkout our predictions!\n",
        "\n",
        "Remember that 0 means **Resistant (R)** and 1 means **Susceptible (S)**"
      ],
      "metadata": {
        "id": "I0Gg3ZGoHbTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the predict() function using the feature combination \"GS\"\n",
        "AMX_GS_labels_pred = predict(DL_AMX_GS_model,AMX_GS_test_array)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\"=0 and \"S\"=1\n",
        "print(\"Labels predicted: \")\n",
        "print(AMX_GS_labels_pred)"
      ],
      "metadata": {
        "id": "DIk2WxCzqTjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "\n",
        "Similarly to the previous notebook, we will evaluate our Deep learning model by using a Confusion Matrix and respective metrics. Below is a quick review of these, remember that there is one Accuracy score, but Recall and Presicion should have as many sets as classes our model its trained to predict:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|\n",
        "|--|:-:|:-:\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(DL_combo_model, features_test, labels_test, labels_pred, cf= True, show_results= True):\n",
        "\n",
        "  labels_test = np.asfarray(labels_test,float)\n",
        "  score = DL_combo_model.evaluate(features_test, to_categorical(labels_test)) # only take accuracy\n",
        "\n",
        "  labels = unique_labels(labels_test, labels_pred)\n",
        "  inp = precision_recall_fscore_support(labels_test, labels_pred, labels=labels, average=None)\n",
        "  report = np.asarray(inp).ravel().tolist()\n",
        "  report= pd.DataFrame(report, index = ['PRC_R','PRC_S','RCL_R','RCL_S','FSc_R','FSc_S','Sc_R','Sc_S'])\n",
        "  report = report.transpose()\n",
        "\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=labels, sample_weight=None)\n",
        "    labels= np.where(labels<1,\"R\",\"S\")\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',score[1])\n",
        "    print('R recall:',report['RCL_R'][0])\n",
        "    print('S recall:',report['RCL_S'][0])\n",
        "    print('R precision:',report['PRC_R'][0])\n",
        "    print('S precision:',report['PRC_S'][0])\n",
        "  return [score[1], report['RCL_R'][0], report['RCL_S'][0], report['PRC_R'][0], report['PRC_S'][0]]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(DL_AMX_GS_model,AMX_GS_test_array, AMX_Train_test_dic['labels_test'],AMX_GS_labels_pred)"
      ],
      "metadata": {
        "id": "wMjJYgGIqboe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) Use all functions and evaluate every drug in every feature combination!**\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**"
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use all our functions this time and save our report into a single data structure\n",
        "DL_model_metrics = {}\n",
        "\n",
        "for drug in drug_list:\n",
        "  print(drug)\n",
        "  Test_Train_dic = Split_train_test(drug) # splits each drug df into a dictionary with testing and training data\n",
        "  for combo in combo_list:\n",
        "    # Training each drug_combo features\n",
        "    labels_train = Test_Train_dic[\"labels_train\"]\n",
        "    features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo) # create corresponding feature_df for training\n",
        "    DL_combo_model = run_DL(features_train, labels_train, drug, combo, view_training = False) # runs deep learning model using the corresponding training feature_df\n",
        "\n",
        "    # Predicting each drug_combo features\n",
        "    features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo) # create corresponding feature_df for testing\n",
        "    labels_pred = predict(DL_combo_model, features_test) # generate predictions based on the feature combination tested\n",
        "\n",
        "    # Evaluating our models\n",
        "    labels_test = Test_Train_dic[\"labels_test\"]\n",
        "    report = evaluate(DL_combo_model,features_test, labels_test, labels_pred, cf=False, show_results= False)\n",
        "    DL_model_metrics[drug+\"_\"+combo] = report\n",
        "\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "glSAeIj2DrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Store the metrics report for all drugs and features combinations as a csv file**"
      ],
      "metadata": {
        "id": "m8CQMKARZ1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dictionary into a dataframe\n",
        "DL_metrics = pd.DataFrame.from_dict(DL_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "DL_metrics = DL_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "# saving our metric results into a CSV file\n",
        "DL_metrics.to_csv(filepath+\"DL_metrics_df.csv\", index= False)\n",
        "DL_metrics\n"
      ],
      "metadata": {
        "id": "VmNdWS65fhRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations for making it this far! We have finally finished creating all our Machine Learning Models and saved the results in different dataframes. Our [Last Notebook](https://colab.research.google.com/drive/1IH7yKbPY0jVUOplj6hdiRM3k470iaabG?usp=sharing) will now create a graph that will help us pick the best results for each of our antibiotics."
      ],
      "metadata": {
        "id": "RUOmATLxR_mD"
      }
    }
  ]
}