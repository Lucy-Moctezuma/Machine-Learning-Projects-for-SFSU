{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/SFSU-CodeLab-Work-/blob/main/E.%20Coli%20Machine%20Learning%20Project/3_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "## ***Objectives for this Notebook***\n",
        "- Familiarize with the basics of how tree models works, specifically Random Forests.\n",
        "- Implementing functions to run Random Forest to Moragadivand's dataset.\n",
        "\n",
        "<a name=\"tree-methods\"></a>\n",
        "Random Forests belong to a group of algorithms known as tree methods. A Random Forest model is also an Ensemble method. These methods use an arrangement of weaker predictor models (in this case Decision Trees) to produce a stronger model (in this case Random Forest). The basis of all tree methods is the Decision Tree. For those unfamiliar with tree methods, we have some definitions below:\n",
        "\n",
        "![tree_forest.png](https://drive.google.com/uc?export=view&id=1TBxIoL7K4WeXhegZZAvURJSxREw-Uqto)\n",
        "\n",
        "- A **Decision Tree (left)** is a supervised learning algorithm that can be used for regression (predicting a continuous variable) or for classification (predicting a categorical variable). Our task is to classify each *E. coli* sample into Resistant (**R**) or Susceptible (**S**) classes. Decision Trees assign values to each feature input and compares them to a calculated threshold. We can picture a tree upside down, starting with the root on top and stopping at the leafs at the bottom, the different squares represent decision thresholds, if the isolate passes the threshold, then we move down the tree following the \"yes\" arrow, if not then we follow the \"no\" arrow to the next square down below until a classification decision is finally made.\n",
        "\n",
        "- A **Random Forest (right)** is the combination of several Decision Trees outputs, each tree classifies all samples independently and the final decision for each sample is made based on the decision of the majority of trees. Each tree inside the Random Forest is created by selecting a random subset of features instead of using all the features and then it uses **BAGGING**.\n",
        "\n",
        "  - **Bagging:** is a process of randomly selecting a subset of unique observations from the training data and then duplicating some of these selections until the number of observations used in one tree is the same as the number of all the original observations in the training data. Essentially it samples from original data with replacement and then agregates results for final prediction. Bagging tries to reduce overfitting and variance.\n",
        "  \n",
        "If we chose to use a combination of features such as: **GS** (Gene Presence or Absence + Population Structure), each tree would randomly use only some columns from the total amount of column features (G + S dataset).\n",
        "\n",
        "We will explain further details on the structure of Random Forests as we code along to further clarify how nodes are chosen and how it uses the outputs of all trees for a final decision.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**"
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import packages for Random Forest model\n",
        "from sklearn.ensemble  import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "import graphviz\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "\n",
        "Similar to the previous notebook we loaded the dataframe created in the first notebook of this tutorial, then we create a dataframe for each antibiotic. To check that our function works we will test it using \"CTX\" antibiotic."
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_dfs.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training (features and labels) and Testing (features and labels)**\n",
        "\n",
        "Similarly to the previous notebook, we create a dataframe and split it into training and testing chunks, each chunk with features and labels. Then we store these 4 sections into a a dictionary. We will then implement this function on the dataframe created in part 2."
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "  df_list = [All_Drugs_df[[\"Isolate\",drug]], All_Drugs_df.iloc[:,13:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "iP4HEW4qBO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we implement the function **Split_train_test()** and then print how many are resistant and how many are Susceptible for the antibiotic CTX."
      ],
      "metadata": {
        "id": "_fAor5uHzNQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test() for CTZ example\n",
        "CTX_Train_test_dic = Split_train_test(\"CTX\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug CTZ\n",
        "print(\"CTX\")\n",
        "for k, df in CTX_Train_test_dic.items():\n",
        "  print(k, df.shape)\n",
        "  # counting how many of the labels have susceptible versus resistant ones\n",
        "  if k.startswith(\"label\"):\n",
        "    print(df.value_counts())"
      ],
      "metadata": {
        "id": "rLhddtjJt9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that just like before we have more *E. coli* samples that are Susceptible, rather than Resistant, therefore we need to remember to pay attention to the metrics of precision and recall for each of these classes rather than just looking at the overall accuracy. **The total number in our training observations is 1245.**"
      ],
      "metadata": {
        "id": "YWUZNSEn-ZEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "**NOTE:** Same code as in previous notebook"
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['G', 'S', 'GY', 'SY', 'GS', 'GYS']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # creating Year column filters for features_df\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Population structure column filters for features_df\n",
        "  pop_str_filter = [col for col in features_df if col.startswith(\"cutoff\")]\n",
        "  pop_struc_feat = features_df[pop_str_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in pop_str_filter and col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'S':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat]\n",
        "    S_feat_df = pd.concat(df_list, axis=1)\n",
        "    S_feat_df = S_feat_df.drop(columns=['Isolate'])\n",
        "    return S_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df\n",
        "\n",
        "  if combo == 'SY':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat, year_feat]\n",
        "    SY_feat_df = pd.concat(df_list, axis=1)\n",
        "    SY_feat_df = SY_feat_df.drop(columns=['Isolate'])\n",
        "    return SY_feat_df\n",
        "\n",
        "  if combo == 'GS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, pop_struc_feat]\n",
        "    GS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GS_feat_df = GS_feat_df.drop(columns=['Isolate'])\n",
        "    return GS_feat_df\n",
        "\n",
        "  if combo == 'GYS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat, pop_struc_feat, ]\n",
        "    GYS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GYS_feat_df = GYS_feat_df.drop(columns=['Isolate'])\n",
        "    return GYS_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "CTX_GYS_train_df = combo_feat(CTX_Train_test_dic['features_train'],\"CTX\",\"GYS\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GYS\" for drug \"CTX\" for training data\n",
        "CTX_GYS_train_df.columns"
      ],
      "metadata": {
        "id": "ERB_2YEXuAsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Random Forest model and training it per feature combination**\n",
        "In this step we can take a closer look at the structure of a Random Forest. First we will create a function that will train our Random Forest to learn to predict Resistance or Susceptibility for a particular strain. Because our function does not specify how many trees to use, the default is 100 Decision Trees created.\n",
        "\n",
        "- We have set a random state in our in the Random Forest Classifier in order to get replicable results for this tutorial."
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Random Forest model function\n",
        "def run_RF(feat_train, lab_train, drug, combo):\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  # creating Random Forest model\n",
        "  RF = RandomForestClassifier(random_state = 42)\n",
        "  # Training Random Forest Model\n",
        "  RF = RF.fit(feat_train, lab_train)\n",
        "  # Checking number of trees in the model\n",
        "  print(\"Number of Decicion Trees in RF model:\", len(RF))\n",
        "  return RF"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will implement our function just as in previous parts, but then we will also take a look at one tree of the Random Forest, after training."
      ],
      "metadata": {
        "id": "58ZumF78K_cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_RF() for specific drug feature combination dataframe\n",
        "RF_CTX_GYS_model = run_RF(CTX_GYS_train_df,CTX_Train_test_dic[\"labels_train\"],\"CTX\",\"GYS\")\n",
        "RF_CTX_GYS_model"
      ],
      "metadata": {
        "id": "jxDDUGtfuCp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have trained our Random Forest model use all features (GYS) to predict for the antibiotic CTX. Below we can graph for example the first tree (index=0). We have 100 trees, and python starts count from 0. Therefore the index would runs from 0 to 99."
      ],
      "metadata": {
        "id": "ZbNGU0btNxoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the index of the tree we are interested in seeing and how many layers to draw\n",
        "index= 0\n",
        "\n",
        "# Drawing the tree by setting the Index and the depth of the tree we would like to draw\n",
        "single_tree = tree.export_graphviz(RF_CTX_GYS_model.estimators_[index], out_file=None,\n",
        "                                  feature_names=CTX_GYS_train_df.columns,\n",
        "                                  filled=True, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth=None)\n",
        "graph = graphviz.Source(single_tree)"
      ],
      "metadata": {
        "id": "wihNsmd3Okiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will store our complete Decision Tree graph as a pdf document. The file will be named based on the index you have chosen to draw, in this case it would be the tree at index 2. Notice that a single tree from our Random Forest can be very big, because the tree will contain several decision thresholds."
      ],
      "metadata": {
        "id": "GAip5wRdkTPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving a single complete decision tree from our Random Forest model into our new directory\n",
        "graph.render(\"/content/drive/MyDrive/EColi_ML_Plots/Classification Tree #: \"+ str(index))"
      ],
      "metadata": {
        "id": "5ZE6BsYLj2wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below on the other hand will let us view only the top part of the tree index 0 (only first 3 layers after the root node), you can control the display of the tree by changing the argument **max_depth**."
      ],
      "metadata": {
        "id": "iqYocJWXhr4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parts of a Decision Tree**:\n",
        "\n",
        "- **Root Node:** displayed at the very top of our Decision tree. It is considered as the most important feature from the random subset of feature columns that our Random Forest (RF) picked. Therefore, each tree in the RF may have a different Root Node.\n",
        "\n",
        "- **Intermediate Nodes:** the first 2 layers of intermediate nodes are being displayed. Similar to the Root Node it contains decision thresholds that eventually lead to a final prediction.\n",
        "\n",
        "- **Leaf Nodes:** Not displayed in the trimmed version, but we can observe them in the pdf. The leaf nodes contain the final predictions within one tree of our RF. You can recognize them because there are no decision thresholds."
      ],
      "metadata": {
        "id": "wDdjIssHyVgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the index of the tree we are interested in seeing and how many layers to draw\n",
        "index= 0\n",
        "# Drawing the tree by setting the Index and the depth of the tree we would like to draw\n",
        "chopped_tree = tree.export_graphviz(RF_CTX_GYS_model.estimators_[index], out_file=None,\n",
        "                                  feature_names=CTX_GYS_train_df.columns,\n",
        "                                  filled=False, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth=2)\n",
        "graph = graphviz.Source(chopped_tree)\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "xgStmSKyjQUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Information within each Node**\n",
        "\n",
        "Below is an schematic for each type of node and uses the first Decision Tree of our Random Forest. The type of information displayed in each node, has been labeled (A, B, C and D) only for the root node but it is present in all the root and intermediate nodes, the leaf nodes only contain B, C and D information. Below we will go over what each of the labeled information represent:\n",
        "\n",
        "![tree-parts.png](https://drive.google.com/uc?export=view&id=1WN05o44tqflVq-GL_2uL_nQuLn3MiNEG)\n",
        "***\n",
        "**A) Decision Threshold:** These are essentially Boolean statement about a column feature that represents a threshold. If the statement is TRUE for a particular observation then we follow the left arrow to and if its false FALSE then we follow the right arrow to the next node. The Decision Threshold is only present in the root and intermediate nodes. In the picture example the thresholds would be: <font face=\"monospace\">cutoff_105 $\\leq$ 151.0 (root node), cutoff_86 $\\leq$ 157.5 (intermediate node), group_3513 $\\leq$ 0.5\n",
        " (intermediate node)</font>, etc\n",
        "\n",
        "The features that start with \"group\" are categorical variables that have already been one-hot encoded, where 1 means that the ortholog gene is present and 0 means that it is not. Therefore if an observation is $\\leq$ 0.5 for those thresholds then it means this gene is not present and we would follow the left arrow.\n",
        "\n",
        "**NOTE:** TRUE is equivalent to **yes (left arrows)** and FALSE equivalent to **no (right arrows)** from the [image](#tree-methods) displayed at the begining of this notebook.\n",
        "\n",
        "***\n",
        "**B) Gini Impurity:** is a measure of purity of the classification. It is a number between 0 and 1, where a value of 0 means that all samples belong to one particular class (purest node); whereas a value of 1 means all samples are distributed equally among the classes (least pure node). Here is a link about [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), if you are interested in the calculation and specific formula behind it.\n",
        "\n",
        "In our graph, we see that the root node has <font face=\"monospace\">Gini = 0.325</font> and as we go lower on the tree the <font face=\"monospace\">Gini = 0.0</font> for the leaf node because the remaining observations in the leaf belong to only one class, the Susceptible (S) one. This means that all the observations that have a <font face=\"monospace\">cutoff_105 $\\leq$ 151.0 </font>(root node) and <font face=\"monospace\">cutoff_86 $\\leq$ 157.5 </font>(intermediate node) are classified exclusively as Susceptible in this particular tree of our Random Forest model.\n",
        "***\n",
        "**C) Samples:** Is the number of unique observations from the original training dataset before the bagging proceedure. In the image we can see that the intermediate node with the threshold cutoff_86 $\\leq$ 157.5 has 93 unique samples and it is later split into a leaf node with 28 unique samples and another intermediate node with 65 samples (93 = 28 + 65).\n",
        "***\n",
        "**D) Values:** is a list of numbers showing the amount of actual observations classified after the bagging proceedure (random sampling of training observations with replacement + aggregation). We only have 2 classes, so the length of this list is just 2 numbers, we can expect a list of 3 if there are 3 classes to classify, 4 if there are 4 classes, etc.\n",
        "\n",
        "In the image we can see that as we go down the decision tree the Resistant observations (in red) in the node with the Decision threshold <font face=\"monospace\">group_3513 $\\leq$ 0.5</font> has <font color=\"red\">164</font> observations classified  as Resistant, then uppon splitting into 2 intermediate nodes this value is also split into <font color=\"red\">59</font> for the node with threshold <font face=\"monospace\">group_19353 $\\leq$ 0.5</font> and <font color=\"red\">105</font>. The same thing happens for the Susceptible split in this node <font color=\"blue\">(938 = 806 + 132) </font>.\n",
        "\n",
        "**NOTE:** Usually for a regular Decision Tree the sum of the list of numbers in values would add up to the sample number BUT in the case of a Random Forest Decision Tree the **Samples** only count the unique observations, whereas our **Values** include the repeated copies from the bagging proceedure as well .\n",
        "***\n",
        "If you are not sure which number corresponds to a particular class, below we can access the index of the values shown in each node. As we can see the first index is Resistant (0) and the second index is Susceptible (1):"
      ],
      "metadata": {
        "id": "igi_iCYchOWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# line to see what classes belong to a particular index in the Values.\n",
        "RF_CTX_GYS_model.classes_"
      ],
      "metadata": {
        "id": "fNT6Xpn6Na5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Random Forest model**\n",
        "\n",
        "The code chunks below will attempt to show us how Random Forests make their predictions using in our case 100 Decision Trees. An important distinction between a Decision Tree and Random Forest is to consider how the trees are generated within the Random Forest. In Random Forest there are 2 randomizations happening when they create the trees:\n",
        "- Random selection of observations from the total training data (rows)\n",
        "- Random selection of features from the feature combination we have chosen, in this case it would be GYS (columns)"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(RF_combo_Model, features_test):\n",
        "  labels_pred = RF_combo_Model.predict(features_test)\n",
        "  return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we implement first the function combo_feat to specify which testing features we want to use to make our predictions. Then we use our predict function to finally get the predictions made by the Random Forest."
      ],
      "metadata": {
        "id": "QG7sFPN6poIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "CTX_GYS_test_df = combo_feat(CTX_Train_test_dic['features_test'],\"CTX\",\"GYS\")\n",
        "\n",
        "# Implementation of the predict() function using the feature combination \"GY\"\n",
        "CTX_GYS_labels_pred = predict(RF_CTX_GYS_model,CTX_GYS_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTX_GYS_labels_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "JqaBG4rPuH7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final predictions from the Random Forests were 100 Resistant and 514 Susceptible. However to see how the final decisions were made, we can check how each tree makes predictions for just the first observation in the features test data. The code below provides a counter for trees that voted Susceptible vs the ones that voted Resistant."
      ],
      "metadata": {
        "id": "22-CAkX6qHYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code below shows the number of trees that predicted one of the two outcomes\n",
        "votes_for_R = 0 #we start with 0 trees \"voting\" for R\n",
        "votes_for_S = 0 #and 0 trees \"voting\" for S\n",
        "\n",
        "#loop over each of the trees in the random forest\n",
        "for i in range(0,len(RF_CTX_GYS_model)):\n",
        "  #get a prediction from one tree\n",
        "  pred = RF_CTX_GYS_model.estimators_[i].predict(CTX_GYS_test_df.loc[0,:].to_numpy().reshape(1,-1))\n",
        "\n",
        "  # if the prediction is 0 then it is \"R\"\n",
        "  if pred == 0.00:\n",
        "    votes_for_R = votes_for_R + 1\n",
        "\n",
        "  #if the prediction is 1 then it is \"S\"\n",
        "  else:\n",
        "    votes_for_S = votes_for_S + 1\n",
        "# Counting the number of trees that predicted R and the ones that predicted S\n",
        "print(\"Number of trees that voted Resistance: \",votes_for_R)\n",
        "print(\"Number of trees that voted Susceptible: \",votes_for_S)"
      ],
      "metadata": {
        "id": "0QBks2vNuFx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can check the prediction by the Random Forest, for the first observation as well. As expected, because the majority of the trees voted for Susceptible (S), we can see that our Random Forest ends up predicting S as well."
      ],
      "metadata": {
        "id": "LvrYcBp4Bozr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction using the second value of our test features\n",
        "pred = RF_CTX_GYS_model.predict(CTX_GYS_test_df.loc[0,:].to_numpy().reshape(1,-1))\n",
        "print(\"Prediction by Random Forest: \", pred)"
      ],
      "metadata": {
        "id": "5-WErPCU0Jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "\n",
        "**a) Creating evaluate() function and implementing it:**\n",
        "\n",
        "Similarly to the previous notebook, we will evaluate our RF model by using a Confusion Matrix and respective metrics. Below is a quick review of these, remember that there is one Accuracy score, but Recall and Presicion should have as many sets as classes our model its trained to predict:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula|<font size=3> Formula for 2 classes|\n",
        "|--|:-:|:-:\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(RF_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=RF_combo_model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=RF_combo_model.classes_)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "  return [report['accuracy'], report['R']['recall'], report['S']['recall'], report['R']['precision'], report['S']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(RF_CTX_GYS_model, CTX_Train_test_dic['labels_test'],CTX_GYS_labels_pred)"
      ],
      "metadata": {
        "id": "sP4N-azGuL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Create a feature importance graph showing which features were the most important**\n",
        "\n",
        "The gini impurity is a metric used to determine which features are more important than others. Below we will learn to graph the feature importance for the model we have been working on."
      ],
      "metadata": {
        "id": "2-6NzvrsbZaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting feature importance for our Random Forest model trained using GYS feature combo for CTX drug\n",
        "importance = RF_CTX_GYS_model.feature_importances_\n",
        "\n",
        "# getting the 20 highest feature importance out of 18291 TOTAL features\n",
        "indices = np.argsort(importance)[-20:]\n",
        "highest_feat_importance = importance[indices]\n",
        "\n",
        "# create a barchart of the 20 most important features\n",
        "sns.set_theme()\n",
        "plt.barh([x for x in range(len(highest_feat_importance))], highest_feat_importance, tick_label=CTX_GYS_train_df.columns[indices])\n",
        "plt.title(\"20 most important features for CTX_GYS_model\")\n",
        "\n",
        "# saving feature importance bargraph and display in notebook\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/RandomForest_CTX_GYS_feat_importance.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TX0HY7tiuRgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the feature importance bar chart it seems that considering all GYS features, the column feature **wbuC** ,which belongs to the Gene Presence or Absence Dataset (**G**), seems to have the highest importance in this particular model for the antibiotic CTX. This probably means that strains with this gene are more likely to be resistant to the drug Cefotaxime (CTX)."
      ],
      "metadata": {
        "id": "z7rCiJS5cXne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) Use all functions and evaluate every drug in every feature combination!**\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "drug_list = All_Drugs_df.iloc[:,1:13].columns\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**"
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use all our functions this time and save our report into a single data structure\n",
        "RF_model_metrics = {}\n",
        "\n",
        "for drug in drug_list:\n",
        "  print(drug)\n",
        "  # splits each drug df into a dictionary with testing and training data\n",
        "  Test_Train_dic = Split_train_test (drug)\n",
        "  for combo in combo_list:\n",
        "    # Training each drug_combo features\n",
        "    labels_train = Test_Train_dic[\"labels_train\"]\n",
        "    # create corresponding feature_df for training\n",
        "    features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "    # runs Random Forest model using the corresponding training feature_df\n",
        "    RF_combo_model = run_RF(features_train, labels_train, drug, combo)\n",
        "\n",
        "    # Predicting each drug_combo features\n",
        "    # create corresponding feature_df for testing\n",
        "    features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "    # generate predictions based on the feature combination tested\n",
        "    labels_pred = predict(RF_combo_model, features_test)\n",
        "\n",
        "    # Evaluating our models\n",
        "    labels_test = Test_Train_dic[\"labels_test\"]\n",
        "    report = evaluate(RF_combo_model, labels_test, labels_pred, cf= False, show_results=False)\n",
        "    RF_model_metrics[drug+\"_\"+combo] = report\n",
        "\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "glSAeIj2DrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **c) Store the metrics report for all drugs and features combinations as a csv file**"
      ],
      "metadata": {
        "id": "m8CQMKARZ1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dictionary into a dataframe\n",
        "RF_metrics = pd.DataFrame.from_dict(RF_model_metrics, orient='index', columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "RF_metrics = RF_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "# saving our metric results into a CSV file\n",
        "RF_metrics.to_csv(filepath+\"RF_metrics_df.csv\", index= False)\n",
        "RF_metrics\n"
      ],
      "metadata": {
        "id": "VmNdWS65fhRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have created and saved all the Random Forest results for every antibiotic and combination of features we were interested in. Now that we have a basic understanding of tree methods in general. It is time to learn about another tree method: [Extreme Gradient Boosted Tree](https://colab.research.google.com/drive/1Ty-l-xSu3t-qbxfx2kN8jMSIOLxC9cYd?usp=sharing)."
      ],
      "metadata": {
        "id": "7b4ILsjdgrN9"
      }
    }
  ]
}