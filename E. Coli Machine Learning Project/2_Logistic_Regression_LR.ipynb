{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/Machine-Learning-Projects-for-SFSU/blob/main/E.%20Coli%20Machine%20Learning%20Project/2_Logistic_Regression_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s_KUXfGSthq"
      },
      "source": [
        "***Click on the button that reads “Open in Colab” to open this code in Google Colab. Once open in Google Colab, you can make a copy of the notebook in your personal drive and run the code by clicking a little triangle/arrow to the left of each code block.***\n",
        "\n",
        "# **Logistic Regression**\n",
        "\n",
        "![LG logo](https://drive.google.com/uc?export=view&id=1XN1JGyxkv5_YsBIFcB0PcOQz2HMcxsmU)\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Learn the basics of how logistic regression model works.\n",
        "- Create functions to implement Logistic Regression to Moradigaravand's dataset.\n",
        "\n",
        "**Logistic Regression** is a classification model that allows us to predict the probability for a binary outcome (2 classes). Typically the threshold for logistic regression is 0.5; In our example, this means that above this probability, the model would predict **R** (Resistant) and below this it will predict **S** (Susceptible).\n",
        "\n",
        "The equation for Logistic Regression is derived from Linear Regression, but but instead of using the response variable Y, it employs the natural log of the odds:\n",
        "\n",
        "**Model Equation (1)**\n",
        "$$ ln(\\frac{P}{1-P}) = \\hat \\beta_0 + \\hat \\beta_jX$$\n",
        "\n",
        "After isolating P, we end up with the equation below:\n",
        "\n",
        "$$P = \\frac{e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}{1+e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}$$\n",
        "\n",
        "- ***P*** is the probability of an outcome. Therefore P is a number between 0(0%) and 1(100%). To make a binary prediction (Resistant or Susceptible), we use a threshold of 0.5. For our example if P < 0.5 our model would predict Susceptible (S) and if P > 0.5 our model would predict Resistant (R)\n",
        "\n",
        "- $\\hat \\beta_0$ is the intercept term and $\\hat \\beta_j = [\\beta_1 , \\beta_2 , \\beta_3, ... , \\beta_{17199}]$ are the coefficients for each of our features, which the model will try to estimate using our data, there is one coefficient per column feature and we are estimating 17199 of them in our example.\n",
        "\n",
        "- $X = [Year \\ column + Gene \\ Absence \\ and \\ Presence \\ columns]$\n",
        "\n",
        "We will see each of the parts of this equation as we go along in the tutorial, so we can have better picture of these.\n",
        "\n",
        "A full understanding of the math behind the logistic regression model is not necessary. Using a logistic regression model (or any other machine learning model) doesn't require a detailed understanding of the math behind it. It is important however to know when to implement particular models. For instance, logistic regression is usually suitable for predicting binary labels, such in this case, where we have to determine either R or S for each isolate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      },
      "source": [
        "### **1) Importing Packages needed**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDa820s0JSoJ",
        "outputId": "9477acc1-05d9-4301-dc84-2b1c75774295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "# Import packages for logistic regression model and hyperparameter tunning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, get_scorer_names\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MTk8vozVJqi"
      },
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "In here we will be loading the CSV we created in the previous notebook. This file should contain information on resistance to all the antibiotic drugs (Labels) and all the Year of isolation , Gene Presence/Absence data (Features)and the Sequence Type data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "psqfFp_yVsmy",
        "outputId": "29a7f90e-1d60-4278-877e-b2273b5f0bdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-69698bc69af7>:6: DtypeWarning: Columns (5,6,7,8,10,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_df.csv\", na_values=\"NaN\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "All_Drugs_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4785f99c-0dbd-43f4-8b76-8b90d619e06b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MLST</th>\n",
              "      <th>Isolate</th>\n",
              "      <th>Year</th>\n",
              "      <th>CTZ</th>\n",
              "      <th>CTX</th>\n",
              "      <th>AMP</th>\n",
              "      <th>AMX</th>\n",
              "      <th>AMC</th>\n",
              "      <th>TZP</th>\n",
              "      <th>CXM</th>\n",
              "      <th>...</th>\n",
              "      <th>group_48768</th>\n",
              "      <th>group_48873</th>\n",
              "      <th>group_48916</th>\n",
              "      <th>group_48933</th>\n",
              "      <th>group_48937</th>\n",
              "      <th>group_48958</th>\n",
              "      <th>group_49020</th>\n",
              "      <th>group_49174</th>\n",
              "      <th>group_49253</th>\n",
              "      <th>group_49257</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ST68</td>\n",
              "      <td>11679_6#21</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>R</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ST652</td>\n",
              "      <td>11658_6#85</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ST95</td>\n",
              "      <td>11657_6#45</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>R</td>\n",
              "      <td>R</td>\n",
              "      <td>R</td>\n",
              "      <td>S</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ST73</td>\n",
              "      <td>11658_4#53</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>R</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ST73</td>\n",
              "      <td>11658_4#54</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>R</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 17213 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4785f99c-0dbd-43f4-8b76-8b90d619e06b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4785f99c-0dbd-43f4-8b76-8b90d619e06b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4785f99c-0dbd-43f4-8b76-8b90d619e06b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d7eb1216-bc4f-41f9-8016-3fd813b7a761\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d7eb1216-bc4f-41f9-8016-3fd813b7a761')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d7eb1216-bc4f-41f9-8016-3fd813b7a761 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    MLST     Isolate    Year CTZ  CTX  AMP AMX AMC TZP CXM  ... group_48768  \\\n",
              "0   ST68  11679_6#21  2001.0   S  NaN  NaN   S   S   S   R  ...           0   \n",
              "1  ST652  11658_6#85  2005.0   S    S  NaN   S   S   S   S  ...           0   \n",
              "2   ST95  11657_6#45  2005.0   S    S  NaN   R   R   R   S  ...           0   \n",
              "3   ST73  11658_4#53  2005.0   S    S  NaN   R   S   S   S  ...           0   \n",
              "4   ST73  11658_4#54  2005.0   S    S  NaN   R   S   S   S  ...           0   \n",
              "\n",
              "  group_48873 group_48916 group_48933 group_48937  group_48958  group_49020  \\\n",
              "0           0           0           0           0            0            0   \n",
              "1           0           0           0           0            0            0   \n",
              "2           0           0           0           0            0            0   \n",
              "3           0           0           0           0            0            0   \n",
              "4           0           0           0           0            0            0   \n",
              "\n",
              "   group_49174  group_49253  group_49257  \n",
              "0            0            0            0  \n",
              "1            0            0            0  \n",
              "2            0            0            0  \n",
              "3            0            0            0  \n",
              "4            0            0            0  \n",
              "\n",
              "[5 rows x 17213 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loads csv file as a dataframe\n",
        "# If the file is \"not found\" go back to Notebook 1 to make sure you create the file in your Drive.\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_df.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMKzeaqPojso"
      },
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training features, training labels, testing features and testing labels.**\n",
        "\n",
        "The objective of this part will be to first create a single dataframe for each antibiotic drug and then split that data frame into 4 parts (see below). The dataframe will have all our features and the label for only one drug. This is because Resistance and Susceptibility are not universal. For example, just because an isolate of *E. coli* is resistant to say AMP (Ampicilin), it doesn't mean that is resistant to AMX (Amoxicilin). We will be training ML models for each of the antibiotics separately.\n",
        "\n",
        "Below we can check the list of antibiotics again:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U40wWYTyjDtv",
        "outputId": "cce3d899-62aa-4e4e-ca28-fb4f0595f698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CTZ', 'CTX', 'AMP', 'AMX', 'AMC', 'TZP', 'CXM', 'CET', 'GEN', 'TBM',\n",
              "       'TMP', 'CIP'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#here we make a list of the antibiotics in our combined dataframe\n",
        "drug_list = All_Drugs_df.iloc[:,3:15].columns\n",
        "drug_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbH90vGNjBGf"
      },
      "source": [
        "After making the individual dataframes for each drug, we will split each of our 12 antibiotic dataframes into 4 different sections:\n",
        "\n",
        "**Two TRAINING sections**\n",
        "\n",
        "**a) labels_train:** are the labels (S or R) for a single antibiotic drug that will be used to teach our model how to make predictions.\n",
        "\n",
        "**b) features_train:** are the features that will be used along with the labels_train to teach our model to make predictions. Note that this is actually the X matrix in our logistic equation! They will be used to estimate our $\\beta_0$ and all the $\\beta_j$ values with a process called *Maximum Likelihood*. You can watch the mathematical details of how this is done by watching this [video](https://www.youtube.com/watch?v=BfKanl1aSG0) by Josh Starmer\n",
        "\n",
        "**Two TESTING sections**\n",
        "\n",
        "**c) labels_test:** are the labels we will holding out so that we can see at the end if we made accurate predictions.\n",
        "\n",
        "**d) features_test:** are the X values we will plug into our model, once $\\beta_0$ and all the $\\beta_j$ values have already been estimated.\n",
        "\n",
        "- Below we create a function that will be used to separate each of our 12 dataframes into the 4 separate parts described above. Specifically we also specified that 20% of our data to be used as a testing set and thus 80% of our data remains to become our training set. You can choose a different percentage to split them, but know that the majority of our data should be used for training.\n",
        "\n",
        "In addition, the function we create to make these 4 sections will save the 4 parts into a python Dictionary object. If you are unfamiliar with what a dictionary is in python, feel free to check out this useful [link](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). This way we can organize and access our 4 data chunks for a specific antibiotic drug.\n",
        "\n",
        "**The function created below will create a dataframe and split the data in Training (labels and features) and Testing (labels and features) for each antibiotic.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R61HHXJCX6Dn"
      },
      "outputs": [],
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test_antibiotic(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 15).\n",
        "  df_list = [All_Drugs_df[[\"MLST\",\"Isolate\",drug,\"Year\"]], All_Drugs_df.iloc[:,15:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  # We use stratify=labels so that the fraction resistant samples is the same in the test and the train sections\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beftysSPkxs_"
      },
      "source": [
        "In the code below we can see what our CTZ dictionary contains the 4 chunks explained at the beggining of part 3. We have 1501 training observations and 376 observations were reserved as testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E26gr3ULdMEC",
        "outputId": "18679ecb-33e4-495d-9fa6-7a9cb1d9cfc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CTZ\n",
            "labels_train (1501,)\n",
            "features_train (1501, 17201)\n",
            "labels_test (376,)\n",
            "features_test (376, 17201)\n"
          ]
        }
      ],
      "source": [
        "# Implementing the function Split_train_test_antibiotic() for CTZ example\n",
        "CTZ_Train_test_dic = Split_train_test_antibiotic(\"CTZ\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug CTZ\n",
        "print(\"CTZ\")\n",
        "for k, df in CTZ_Train_test_dic.items():\n",
        "  print(k, df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIcTBpVQRSya"
      },
      "source": [
        "Below we can see a count of Susceptible and Resistance strains for the Training and Testing datasets. Notice that there are a lot more Susceptible than Resistant *E. coli* isolates to CTZ drug. This is good news for us as humans (most *E. coli* are susceptible to most drugs), but for ML approaches it can be a problem. This is considered an \"inbalanced\" dataset. Later on we will discuss how certain metrics may not be as reliable as others when looking at an imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKGS4Ex4AtU",
        "outputId": "83b9f1dd-ed9c-4cb2-95ed-3575584f5884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Counts for Training Dataset:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CTZ\n",
              "S    1295\n",
              "R     206\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Training Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_train\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qumUgwdy4vUA",
        "outputId": "f23678f2-d6e1-46b0-beb3-7bc60fb0c853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Counts for Testing Dataset:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CTZ\n",
              "S    324\n",
              "R     52\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Testing Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_test\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IyqN506dQn4"
      },
      "source": [
        "### **4) Creating different combinations of features before training**\n",
        "\n",
        "The next part of this project is to add some complexity in our analysis by choosing specifically what sort of features we would like to include.\n",
        "\n",
        "Recall that we have 2 types of features we went on detail on our [first notebook](https://colab.research.google.com/drive/13SbCF3LFXwM_jZELxQA4aBHUvPgfmzzP?usp=sharing):\n",
        "\n",
        "- **Y**: Years of Isolation\n",
        "- **G**: Gene presence and absence\n",
        "\n",
        "We are interested in the following combinations: **G, Y** and **GY** This means that for any possible combination of feautures, we will train and test the machine learning models.\n",
        "\n",
        "Below we create a function that will take the features dataframe (train or test) from the dictionary we have created in part 3 and then create dataframes with different feature combinations ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "outputs": [],
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['Y', 'G', 'GY']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # Isolating Year as a feature and\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'Y':\n",
        "    df_list = [features_df[['MLST','Isolate']], year_feat]\n",
        "    Y_feat_df = pd.concat(df_list, axis=1)\n",
        "    Y_feat_df = Y_feat_df.drop(columns=['Isolate'])\n",
        "    return Y_feat_df\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], year_feat, gene_presc_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqvvdtJOu_RK",
        "outputId": "45ea267a-45cf-49a9-9928-12bb55b048eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Year', 'MLST', 'yeiU', 'yhhS', 'ybaE', 'eutR', 'ibrB', 'ytfP', 'aslB',\n",
              "       'narQ',\n",
              "       ...\n",
              "       'group_48768', 'group_48873', 'group_48916', 'group_48933',\n",
              "       'group_48937', 'group_48958', 'group_49020', 'group_49174',\n",
              "       'group_49253', 'group_49257'],\n",
              "      dtype='object', length=17200)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "CTZ_GY_train_feat = combo_feat(CTZ_Train_test_dic['features_train'],\"CTZ\",\"GY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GY\" for drug \"CTZ\" for training data\n",
        "CTZ_GY_train_feat.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rnDaATPjkiy"
      },
      "source": [
        "**NOTES:**\n",
        "\n",
        "- We have **MLST** Sequence Type as a possible feature, but we will not use it as a feature in this tutorial. We will drop it for training the model.\n",
        "\n",
        "- Later we can choose to use MLST for cross-validation purposes in order to increase generalizability of the ML models and avoid overfitting. This is important to consider because isolates that might have come from the same place, patient, etc. might have similar genetic composition and thus not be independent from each other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      },
      "source": [
        "### **5) Creating and Running Logistic regression model**\n",
        "\n",
        "The next step involves creating a function that will actually create our Logistic Regression model and train it on our desired combination of training features and the labels for the drug we choose. While this function seems fairly straight forward, there is a lot of calculations happening in the backrgound when we call the LG.fit() function which trains our model. Notice that in this example function we are not considering the **MLST** column. We will only take a small peak at what's going on in the background.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "outputs": [],
      "source": [
        "# creating Logistic regression model function\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def run_LG(feat_train_df, lab_train, drug, combo):\n",
        "  feat=feat_train_df.drop(columns=['MLST'])\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  LG = LogisticRegression(random_state = 42, max_iter=500, class_weight='balanced')\n",
        "  return LG.fit(feat, lab_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "_HYfVIgUvEWd",
        "outputId": "11371f40-e1c3-4e18-f4f4-7107cd775e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CTZ Training combo: GY\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(class_weight='balanced', max_iter=500, random_state=42)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# implementing run_LG() for specific drug feature combination dataframe\n",
        "LG_CTZ_GY_model = run_LG(CTZ_GY_train_feat, CTZ_Train_test_dic['labels_train'],\"CTZ\",\"GY\")\n",
        "LG_CTZ_GY_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt-5wtNNV7l3",
        "outputId": "842b5a26-2015-49de-92a4-d1ec2e194787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['R', 'S'], dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# looking at what classes the model has used to make predictions\n",
        "LG_CTZ_GY_model.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynlnJ_21mH7a"
      },
      "source": [
        "Below we can see that our model contains several coefficients. We will printout first the intercept ($\\hat{\\beta}_{0}$), then a list of the coefficients that correspond to each column feature ($\\hat{\\beta}_{j}$) and finally we can get a print out of the total number of coefficients in our model, these should be the same number as all the features we used in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t99qNo2bvF8Z",
        "outputId": "a2e2d893-e7ff-4fe7-fa0b-29ef09246549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: 0.003850673046459656\n",
            "All beta_j values: [-0.00155736  0.00360435 -0.00056664 ...  0.          0.\n",
            "  0.        ]\n",
            "Number of beta_j values:  17199\n"
          ]
        }
      ],
      "source": [
        "# printing the beta_0 or intercept value of our model\n",
        "print(\"Intercept:\",LG_CTZ_GY_model.intercept_[0])\n",
        "\n",
        "# printing all the beta_j's or coefficients of our logistic regression model\n",
        "print(\"All beta_j values:\", LG_CTZ_GY_model.coef_[0])\n",
        "\n",
        "# printing the number of all the beta_j values\n",
        "print(\"Number of beta_j values: \", len(LG_CTZ_GY_model.coef_[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1px3jn-Djn77"
      },
      "source": [
        "### **6) Making predictions from Logistic regression model**\n",
        "\n",
        "Now that our model has been trained and all $\\beta$ values have been estimated, we are ready to make predictions! We will use the features of our testing data which we separated when we made our antibiotic drug dictionary.\n",
        "\n",
        "Below we create another function where we predict labels using the actual model and the \"features_test\" chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(LG_combo_Model, features_test):\n",
        "  feat = features_test.drop(columns=['MLST'])\n",
        "  labels_pred = LG_combo_Model.predict(feat)\n",
        "  if is_numeric_dtype(labels_pred):\n",
        "    # tranforming labels from numbers to letters\n",
        "    lab_pred_t = labels_pred.astype('O')\n",
        "    lab_pred_t[lab_pred_t==0] = 'R'\n",
        "    lab_pred_t[lab_pred_t==1] = 'S'\n",
        "    return lab_pred_t\n",
        "  else:\n",
        "    return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwYUz_D4Un57"
      },
      "source": [
        "Below we will use the function **combo_feat()** to split the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xifjI6iHvK66"
      },
      "outputs": [],
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "CTZ_GY_test_df = combo_feat(CTZ_Train_test_dic['features_test'],\"CTZ\",\"GY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYZfi84wmweT"
      },
      "source": [
        "The we will implement the function **predict()** below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE1fy63XvI0X",
        "outputId": "9747ec7f-f8e0-4efd-ef17-e239ce84aeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels predicted:  (array(['R', 'S'], dtype=object), array([ 42, 334]))\n"
          ]
        }
      ],
      "source": [
        "# Implementation of the predict() function using the feature combination \"GY\"\n",
        "CTZ_GY_labels_pred = predict(LG_CTZ_GY_model,CTZ_GY_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTZ_GY_labels_pred, return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi1DXXxfwUzd"
      },
      "source": [
        "We can see in the last output that index order for **R (Resistance is 0)** and for **S (Susceptible is 1)**, meaning that we predicted 42 Resistant and 334 Susceptible samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QGUYMhg5Gy9",
        "outputId": "19d942a9-332f-461f-d63c-56f2467abdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels predicted for first 10 test isolates:  ['S' 'S' 'S' 'R' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'R' 'S' 'S'\n",
            " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'R' 'S' 'S' 'S' 'S']\n",
            "Labels predicted for first 10 test isolates:  ['S' 'S' 'S' 'R' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'R' 'S' 'S'\n",
            " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'R' 'S' 'S' 'S' 'S']\n"
          ]
        }
      ],
      "source": [
        "# let's look at the predictions for the first 30 isolates\n",
        "print(\"Labels predicted for first 10 test isolates: \", CTZ_GY_labels_pred[:30])\n",
        "\n",
        "# and the actual labels for the first 30 isolates – do they match?\n",
        "print(\"Labels predicted for first 10 test isolates: \", np.array(CTZ_Train_test_dic['labels_test'][:30]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dox3JfFn5wz"
      },
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "There are different ways we can evaluate our model. A **confusion matrix** is a plot showing a prediction summary for our model. It allows us to see how many predictions were correct and incorrect. There are also different metrics we can calculate using this graph. For this tutorial we will focus on three of them: **Accuracy**, **Recall** and **Precision**.\n",
        "\n",
        "- **Accuracy:** is the total number of correct classifications over the total amount of predictions made.\n",
        "\n",
        "- **Recall:** is the number of correct classifications made for a particular class over all predictions of that class.\n",
        "\n",
        "- **Presicion:** is the number of classifications made for a particular class over the actual number of strains for that class.\n",
        "\n",
        "Recall and Precision can each be calculated for resistance and for susceptibility.\n",
        "\n",
        "When we have two classes, a 2 by 2 confusion matrix contains:\n",
        "\n",
        "- **True Positives (TP)** = Resistant strains correctly classified as resistant (R) = 34\n",
        "- **True Negatives (TN)** = Susceptible strains correctly classified as susceptible (S) = 316\n",
        "- **False Positives (FP)** = Susceptoble strains incorrectly classified as resistant (R) = 8\n",
        "- **False Negatives (FN)** = Resistant strains incorrectly classified as susceptible (S) = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "outputs": [],
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(LG_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=np.unique(labels_pred))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(labels_pred))\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print(\"Metrics for each class:\")\n",
        "    print(\"Resistant (R)\")\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('R f1-score:',report['R']['f1-score'])\n",
        "    print(\"Susceptible (S)\")\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "    print('S f1-score:',report['S']['f1-score'])\n",
        "\n",
        "  return [report['accuracy'], report['R']['recall'], report['R']['precision'], report['R']['f1-score'], report['S']['recall'], report['S']['precision'], report['S']['f1-score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0uqIS4V0Il4"
      },
      "source": [
        "Before we implement our function we can show a manual way in which these metrics are calculated, first for the overall *Accuracy* and then *Recall* and *Precision* for only the Resistant strains (R) as an example:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|<font size=3>Manual Calculation|\n",
        "|--|:-:|:-:|:--|\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|<font size=2>$\\frac{34 + 316}{376} = 0.931$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|<font size=2>$\\frac{34}{34 + 18} = 0.654$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|<font size=2>$\\frac{34}{34 + 8} = 0.810$|\n",
        "|<font size=3>**R F1-score:**|<font size=3>$\\frac{2(R_{Precision})(R_{Recall})}{R_{Precision} + R_{Recall}}$|<font size=3>$\\frac{2*TP}{2*TP + FP + FN}$|<font size=2>$\\frac{2*34}{2*34 + 8 + 18} = 0.723$|\n",
        "\n",
        "**NOTE:** In this tutorial we only work with 2 classes (R and S), thus the abbreviations (TP, TN, FP and FN) apply to our confusion matrix, however for situations with more than 2 classes, refer to the general formula column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "AM3fq7QqvNqh",
        "outputId": "149a32ae-7f70-4b58-ee80-bb574d0b3891"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results\n",
            "Accuracy: 0.9308510638297872\n",
            "Metrics for each class:\n",
            "Resistant (R)\n",
            "R recall: 0.6538461538461539\n",
            "R precision: 0.8095238095238095\n",
            "R f1-score: 0.7234042553191489\n",
            "Susceptible (S)\n",
            "S recall: 0.9753086419753086\n",
            "S precision: 0.9461077844311377\n",
            "S f1-score: 0.9604863221884499\n"
          ]
        }
      ],
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(LG_CTZ_GY_model, CTZ_Train_test_dic['labels_test'], CTZ_GY_labels_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNSVkF2G9bW8"
      },
      "source": [
        "Note that in this example, accuracy is 93.09%. That sounds quite good. But 18 out of the 52 of the resistant cases were misclassified as susceptible (recall score for R). So, if the main goal here is to detect resistant strains, this particular model does not perform so well. Notice for R metrics, precision measures the accuracy of R predictions specifically, while recall measures completeness of R predictions. There is usually an inverse relationship between recall and precision, even though ideally we want both metrics to be close to 1. If there is no preference and we deem both metrics to be equally as important then we can look at the f1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GHhuszf-9Ka"
      },
      "source": [
        "### **8) Hyperparameter Tuning and Crossvalidation for LG model**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onoYJmwjEdsw"
      },
      "source": [
        "\n",
        "#### **a) Hyperparameter Tunning**\n",
        "This is the process of modifying the way we want to train our data, every ML model has different hyperparameters we can tune in order to get the best results. The LG model used several default parameters that we could tune. For a complete list of hyperparameters and details of what each controls for LG model, click [here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "However, to keep this tutorial simpler we will perform tuning for just the hyperparameter **\"C\"**, which controls the amount of regularization.\n",
        "\n",
        "**Regularization:** Is a general term that means controlling how much weight we put on our training data. If we put a lot of weight in training data, then our model will learn very well our train data but will have some trouble predicting new unseen data, therefore it may fail to generalize. This issue is generally known as **overfitting**. There are many ways to perform regularization in different ML models. In this case the parameter **\"C\"** is the inverse of regularization strength ($\\lambda$). By default sklearn uses \"l2\" (Ridge regression) as the penalty type (another hyperparameter). This **C** (C = $\\frac{1}{\\lambda}$) hyperparameter can be seen when calculating the Residual Sum of Squares (RSS), which is a way to measure how well a regression model matches the training data.\n",
        "\n",
        "Therefore in this tutorial, the RSS equation it follows is:\n",
        "\n",
        "$$ RSS_{l2} = -ln(L) + \\lambda * \\sum^p_{j=1}\\beta^2_j $$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$ -ln(L) = -\\sum^N_{i=1}[-ln(1 + e^{(\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X)}) + y_i(\\hat{\\beta_{0}}+\\hat{\\beta}_{j}X)] $$\n",
        "\n",
        "The equation $-ln(L)$ is known as a loss function. A loss function is one that attempts to quantify how well a model fits the training data. A bigger number means it is not a good fit and a smaller number means it is a good one.\n",
        "\n",
        "Loss functions differ depending on the ML model as they are derived from the model equation. In this case this was derived from the logistic regression equation (1) at the beggining of this notebook. For more mathematical details, check [here](https://compgenomr.github.io/book/logistic-regression-and-regularization.html#eq:llog).\n",
        "\n",
        "When we look at RSS equation, the same principle applies , we want a low RSS number in general, however the second term $\\lambda * \\sum^p_{j=1}\\beta^2_j$  is known as the **l2 penalty**, and this term essentially increases the results of our loss function, that way we stop it from overfitting. We don't want the RSS to ever become 0, because that would mean perfect preditions for training data. By applying a penalty here it regulates and makes sure that it is not 0 and we do not overfit. Since hyperparameter **C** is the inverse of $\\lambda$, we can clearly see that a smaller C means a higher lambda which makes RSS higher and thus not overfit.\n",
        "\n",
        "We start first by creating a dictionary of the parameters we would like to tune in our logistic regression model. The default C in sklearn is 1, but we can test 4 different values, from more to less regularization strength:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcwghBhUwgn6"
      },
      "outputs": [],
      "source": [
        "# performing hyperparameter tunning for model\n",
        "## Creating dictionary of parameters to tune.\n",
        "hparam = {\"C\":[0.001, 0.01, 0.1, 1.00]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw_n7OIQwoaN"
      },
      "source": [
        "#### **Crossvalidation Type**\n",
        "##### **b) Random F-fold Crossvalidation**\n",
        "\n",
        "Crossvalidation is a technique used during training to avoid overfitting. In it's simplest form the training dataset splits into different chunks of data, where each chunk takes turns to become a hold out chunk of data to be tested after training with the remaining chunks.\n",
        "\n",
        "There are different crossvalidation schemes. Some research papers within the field of population genetics prefer to take into account the population structure of the dataset, as several isolates may belong to the same sequence type, patient or phylogenetic group, therefore they prefer to apply crossvalidation schemes that perform group splits crossvalidation. Here is a list of [different crossvalidation schemes](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) available in sklearn.\n",
        "Others choose to reweight isolates depending on their group size, in order to give less importance to samples from the same group.\n",
        "\n",
        "\n",
        "The function created below will be used to perform the **hyperparameter \"C\" tunning** using a regular **4-fold crossvalidation** scheme, meaning the training data will be randomly split in 4 chunks, where each of the chunks will take turns to be the a validation test set, while the remaining chunks are used for training. Therefore we will train 16 times, (4 values for \"C\" X 4 validation tests). The output of our function will be the best model found after the hyper parameter tunning and crossvalidation performed.\n",
        "\n",
        "##### **c) Blocked Crossvalidation**\n",
        "\n",
        "As mentioned before we are not using Sequence Type to perform crossvalidation , however if we did, it would be called a \"Blocked Crossvalidation\". This is a way to performs crossvalidation taking into account that some isolates might be highly related to others and therfore in order to keep our model as generalizable as possible, the training data is trained in a set of isolates belonging to  a particular ST but the validation testing is performed with isolates of a different set of St groups. This technique has been applied for example in this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7059009/).\n",
        "\n",
        "There is not a consensus in terms of how to deal with genetically related isolates. Ideally we want all isolates drawn to be independent. Blocked crossvalidation is not the only way to deal with it. For more information you can refer to the paper that accompanies this tutorial.\n",
        "\n",
        "Below we present the code to perform **blocked crossvalidation**, we could potentially include the **MLST** column as the blocking factor. Others population structure factors used for blocking have been phylogenetical groups or SNP cutoff values, etc. We will not be implementing this method for our final figure, however the code is provided below for you to have the option to perform it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV7gsnwnAAU3"
      },
      "outputs": [],
      "source": [
        "# Creating function to perform hyper parameter tunning of model\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def LG_hp_tune(param, feat_train_df, lab_train, v=3, cv=4):\n",
        "  #creating logistic regression model\n",
        "  model = LogisticRegression(random_state = 42, max_iter=500, class_weight='balanced', n_jobs=2)\n",
        "  # performing the hyper parameter tunning using crossvalidation\n",
        "  scoring_dic = {'f1_macro':make_scorer(f1_score , average='macro')}\n",
        "  # isolating the features used for training\n",
        "  feat = feat_train_df.drop(columns=[\"MLST\"])\n",
        "  # tranforming labels from letters to 0 & 1\n",
        "  lab_train_t = lab_train.replace({'R': 0, 'S': 1})\n",
        "  # Fitting using grid search parameters depending on crossvalidation scheme used\n",
        "  if str(cv).isnumeric():\n",
        "    cv = KFold(cv)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t)\n",
        "  elif cv == \"blocked\":\n",
        "    groups= feat_train_df['MLST']\n",
        "    cv = StratifiedGroupKFold(n_splits=4)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t, groups=groups)\n",
        "  else:\n",
        "    print(\"Please provide a valid crossvalidation scheme `blocked` or an integer\")\n",
        "  print(gs.best_params_)\n",
        "  print(gs.best_score_)\n",
        "  return gs.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVLRjlzIoSxe",
        "outputId": "60821ee2-2502-419c-a434-12c4584bf136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
            "[CV 1/4] END ....C=0.001; f1_macro: (train=0.735, test=0.751) total time=   9.7s\n",
            "[CV 2/4] END ....C=0.001; f1_macro: (train=0.730, test=0.678) total time=   8.8s\n",
            "[CV 3/4] END ....C=0.001; f1_macro: (train=0.740, test=0.665) total time=   9.9s\n",
            "[CV 4/4] END ....C=0.001; f1_macro: (train=0.731, test=0.675) total time=   6.8s\n",
            "[CV 1/4] END .....C=0.01; f1_macro: (train=0.904, test=0.840) total time=  25.8s\n",
            "[CV 2/4] END .....C=0.01; f1_macro: (train=0.899, test=0.795) total time=  22.6s\n",
            "[CV 3/4] END .....C=0.01; f1_macro: (train=0.910, test=0.711) total time=  16.2s\n",
            "[CV 4/4] END .....C=0.01; f1_macro: (train=0.922, test=0.792) total time=  24.8s\n",
            "[CV 1/4] END ......C=0.1; f1_macro: (train=0.984, test=0.867) total time=  25.8s\n",
            "[CV 2/4] END ......C=0.1; f1_macro: (train=0.979, test=0.824) total time=  27.6s\n",
            "[CV 3/4] END ......C=0.1; f1_macro: (train=0.984, test=0.765) total time=  25.8s\n",
            "[CV 4/4] END ......C=0.1; f1_macro: (train=0.988, test=0.827) total time=  25.5s\n",
            "[CV 1/4] END ......C=1.0; f1_macro: (train=1.000, test=0.853) total time=  23.7s\n",
            "[CV 2/4] END ......C=1.0; f1_macro: (train=0.996, test=0.833) total time=  27.5s\n",
            "[CV 3/4] END ......C=1.0; f1_macro: (train=0.991, test=0.749) total time=  27.8s\n",
            "[CV 4/4] END ......C=1.0; f1_macro: (train=0.998, test=0.805) total time=  25.3s\n",
            "{'C': 0.1}\n",
            "0.8207275585127436\n"
          ]
        }
      ],
      "source": [
        "# Running the function created (takes long time)\n",
        "LG_tunned = LG_hp_tune(hparam, CTZ_GY_train_feat, CTZ_Train_test_dic['labels_train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4F5eaZTj-d"
      },
      "source": [
        "The function created used **f1 score** to search for the best C parameter. The f1 score is a more appropiate score to interpret results where there is an imbalance in classes, since there are more S (1295) than R (206) isolates for CTZ in our dataset. We used the **f1_score macro average**, which takes into account the f1 scores for each class (R & S) and averages them. f1 scores range from 0 to 1.\n",
        "\n",
        "- At the end of our crossvalidation it shows that our best C parameter is 0.1\n",
        "- The highest average f1 score macro obtained from both of the classes is 0.821\n",
        "\n",
        "We can now use the improved (hyper parameter tuned ) model to predict labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjQ2FO9ub-po",
        "outputId": "02e0f540-4719-4489-8672-0464cd341bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels predicted:  (array(['R', 'S'], dtype=object), array([ 45, 331]))\n"
          ]
        }
      ],
      "source": [
        "# making predictions with best model from hyperparameter tunning\n",
        "CTZ_GY_tunned_pred = predict(LG_tunned,CTZ_GY_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTZ_GY_tunned_pred, return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuIRJNsm4ewC"
      },
      "source": [
        "Below we can compare the evaluation between the model before and after tunning to see if there are any differences:\n",
        "\n",
        "**BEFORE Hyperparameter Tunning**\n",
        "\n",
        "Default C = 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auGCY4pl43C0",
        "outputId": "8685dc94-9546-40f2-9eee-1c3f31c29eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results\n",
            "Accuracy: 0.9308510638297872\n",
            "Metrics for each class:\n",
            "Resistant (R)\n",
            "R recall: 0.6538461538461539\n",
            "R precision: 0.8095238095238095\n",
            "R f1-score: 0.7234042553191489\n",
            "Susceptible (S)\n",
            "S recall: 0.9753086419753086\n",
            "S precision: 0.9461077844311377\n",
            "S f1-score: 0.9604863221884499\n"
          ]
        }
      ],
      "source": [
        "# Evaluation for LG model before tunning\n",
        "Model_Report = evaluate(LG_CTZ_GY_model, CTZ_Train_test_dic['labels_test'], CTZ_GY_labels_pred, cf=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccr8QygZHZG1"
      },
      "source": [
        "**AFTER Hyperparameter Tunning**\n",
        "\n",
        "Tunned C = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOqCxxOt5C1D",
        "outputId": "791330f0-f0be-4ae7-bfc6-2edc6abc1b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results\n",
            "Accuracy: 0.9281914893617021\n",
            "Metrics for each class:\n",
            "Resistant (R)\n",
            "R recall: 0.6730769230769231\n",
            "R precision: 0.7777777777777778\n",
            "R f1-score: 0.7216494845360825\n",
            "Susceptible (S)\n",
            "S recall: 0.9691358024691358\n",
            "S precision: 0.9486404833836858\n",
            "S f1-score: 0.9587786259541985\n"
          ]
        }
      ],
      "source": [
        "# Evaluation for LG model AFTER tunning\n",
        "Tunned_Model_Report = evaluate(LG_tunned, CTZ_Train_test_dic['labels_test'], CTZ_GY_tunned_pred, cf=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4KMBQ_uHgS1"
      },
      "source": [
        "Above we see the difference between the regular vs the tuned model, we see that the scores do not defer dramatically, except that R recall is a bit better now. Take into account that we have only performed tuning on one hyperparameter, perhaps if we did tuning for more hyperparameters our results could have been even better, however for the purposes of this tutorial, we will leave it at that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5lGr0hoDCip"
      },
      "source": [
        "### **9) Use functions created and evaluate every drug in every feature combination!**\n",
        "\n",
        "In this part we will combine all the functions we have created in order to get the results for different drug and feature combinations used to train our new tunned Logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yExXAdaWZL4_"
      },
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51wJM2XUaxtQ",
        "outputId": "8680dca6-23ed-4ec4-b725-91eac8cca5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['CTZ', 'CTX', 'AMP', 'AMX', 'AMC', 'TZP', 'CXM', 'CET', 'GEN', 'TBM',\n",
            "       'TMP', 'CIP'],\n",
            "      dtype='object')\n",
            "['Y', 'G', 'GY']\n"
          ]
        }
      ],
      "source": [
        "# let's check all drugs\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all feature combinations we are interested in\n",
        "print(combo_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWO1rYocRQR"
      },
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**\n",
        "\n",
        "The code below will take fairly long to run because we will train and hyperparameter tune several logistic regression models using different feature combinations for each of the 12 antibiotic drugs. It will then be stored in a python dictionary and dataframe so it can be accessed later.\n",
        "\n",
        "**NOTE:** Python Dictionaries, are common data structures within the python language that are very efficient at storing and retrieving information. It consists in key-value pairs, in our case each key is: drug_combo and the value is paired with are the metrics results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "glSAeIj2DrnP"
      },
      "outputs": [],
      "source": [
        "for fname in os.listdir(filepath):\n",
        "    if fname.endswith('Logistic_Regression_Results_df.csv'):\n",
        "        print(\"A csv with stored results for Logistic Regression has already been created. Please check your Google Drive directory.\")\n",
        "        break\n",
        "else:\n",
        "  # Lets use all our functions this time and save our report into a single data structure\n",
        "  LG_model_metrics = {}\n",
        "\n",
        "  for drug in drug_list:\n",
        "    print(drug)\n",
        "    # splits each drug df into a dictionary with testing and training data\n",
        "    Test_Train_dic = Split_train_test_antibiotic(drug)\n",
        "    for combo in combo_list:\n",
        "      # Training each drug and combo features\n",
        "      labels_train = Test_Train_dic[\"labels_train\"]\n",
        "      # create corresponding feature_df for training\n",
        "      features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "      print(drug+\"_\"+combo)\n",
        "\n",
        "      #Hyperparamter tuning and final model creation with tunned parameters:\n",
        "      LG_combo_model = LG_hp_tune(hparam, features_train, labels_train, v=0)\n",
        "\n",
        "      # create corresponding feature_df for testing\n",
        "      features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "      # generate predictions based on the feature combination tested\n",
        "      labels_pred = predict(LG_combo_model, features_test)\n",
        "      # loading the actual labels\n",
        "      labels_test = Test_Train_dic[\"labels_test\"]\n",
        "      # Evaluating our model\n",
        "      report = evaluate(LG_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "      # Saving the results into a dictionary\n",
        "      LG_model_metrics[drug+\"_\"+combo] = report\n",
        "      print(report)\n",
        "  # convert dictionary into a dataframe\n",
        "  LG_metrics = pd.DataFrame.from_dict(LG_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"R_precision\", \"R_f1_score\", \"S_recall\", \"S_precision\", \"S_f1_score\"]).reset_index()\n",
        "  LG_metrics = LG_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "  # saving our metric results into a CSV file\n",
        "  LG_metrics.to_csv(filepath+\"Logistic_Regression_Results_df.csv\", index= False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-6NzvrsbZaH"
      },
      "source": [
        "### **10) Create a bar graph showing metrics for all drugs using GY combination of features**\n",
        "\n",
        "Using the CSV we created above with all our results, we will create a bargraph to visualize metrics for every antibiotic that uses the feature combination GY (Gene Presence and Absence & Year of Isolate collection). In the graph below we are comparing Accuracy and Recall scores for both classes.\n",
        "\n",
        "The code below serves to create a new directory to store all our Machine Learning plots and figures created. If you run the code more than once it will let you know that the figure has already been created in the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQs9pOMYvUg1"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # makes a directory for all your plot images\n",
        "  os.mkdir('/content/drive/My Drive/EColi_ML_Plots')\n",
        "except:\n",
        "  print(\"A directory was already created to store your plot\")\n",
        "\n",
        "# filtering for all the rows that contain GY features\n",
        "LG_metrics = pd.read_csv(\"/content/drive/MyDrive/EColi_ML_CSV_files/LG_metrics_df.csv\")\n",
        "GY_filter = [drug_combo for drug_combo in LG_metrics['Drug_combo'] if drug_combo.endswith(\"GY\")]\n",
        "GY_df = LG_metrics.loc[LG_metrics[\"Drug_combo\"].isin(GY_filter)]\n",
        "\n",
        "# Figure Size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize =(15, 6))\n",
        "\n",
        "# Adding title\n",
        "plt.title('Accuracy, R_recall, S_recall', fontsize = 12)\n",
        "\n",
        "# Variables to be plotted\n",
        "x = np.arange(len(GY_df[\"Drug_combo\"]))\n",
        "acc = list(GY_df[\"Accuracy\"])\n",
        "R_rec = list(GY_df[\"R_recall\"])\n",
        "S_rec = list(GY_df[\"S_recall\"])\n",
        "\n",
        "# Plotting barcharts\n",
        "acc_bar=plt.bar(x-0.25, height= acc, width=0.25, color=\"grey\", edgecolor=\"gray\")\n",
        "rrec_bar=plt.bar(x, height= R_rec, width=0.25, color=\"plum\", edgecolor=\"gray\")\n",
        "srec_bar=plt.bar(x+0.25, height= S_rec, width=0.25, color=\"lavenderblush\", edgecolor=\"gray\")\n",
        "\n",
        "plt.xticks([r for r in range(len(GY_df[\"Drug_combo\"]))],\n",
        "            GY_df[\"Drug_combo\"], fontsize = 12)\n",
        "\n",
        "#legend\n",
        "fig.legend([acc_bar,rrec_bar,srec_bar],[\"Accuracy\", \"R_recall\", \"S_recall\"], bbox_to_anchor=(0.4,-0.35, 0.04, 0.4), fontsize=12)\n",
        "\n",
        "# Saving bargraph into our new directory\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/LG_GY_Accuracy_and_Recall_Scores.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "# Show Plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXsvRlZk4fxU"
      },
      "source": [
        "Notice for instance the antibiotic **CTZ**, when we only pay attention to the Accuracy, we would conclude that our model is doing fairly well, however, when examine recall we can clearly see that the Recall for Resistance is noticeably lower than for Susceptibility. Therefore, for imbalanced classes, Accuracy is usually not the best metric to use.\n",
        "\n",
        "The choice of giving more importance to Recall vs Precision depends on the situation, for example if we were trying to make predictions whether a person should be screened for stomach cancer vs not. We would care more about Recall because we want to err on the safest side (detect as many of the potentially cancer cases as possible).\n",
        "\n",
        "***OPEN QUESTION:***\n",
        "\n",
        "Look at the graph. Is recall typically better for R or S? why do you think that’s the case?\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xngjGG3BF47x"
      },
      "source": [
        "\n",
        "Thanks for making it this far! We have accomplished the objectives of this notebook. We learned the basics of how logistic regression works and learned how to make functions in order to run this model using Moragadivand's *E. coli* dataset. Now we will move on to our next ML model, [Random Forest](https://colab.research.google.com/drive/1ptlu-cy3RRavRbLQseNH5HKzcPTcD-Hf?usp=sharing) and learn a bit more about tree-based models in general."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}