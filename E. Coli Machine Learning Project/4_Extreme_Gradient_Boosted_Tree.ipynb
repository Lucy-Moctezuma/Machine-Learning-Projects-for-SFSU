{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/SFSU-CodeLab-Work-/blob/main/E.%20Coli%20Machine%20Learning%20Project/4_Extreme_Gradient_Boosted_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extreme Gradient Boosted Tree**\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Expand concepts learned about tree methods (ensemble methods) into Extreme Gradient Boosted Trees Algorithm.\n",
        "- Create functions to implement Gradient Boosted Trees into Moragadivand's dataset.\n",
        "\n",
        "**Extreme Gradient Boosted Tree (XGBoost)** is another ensemble method similar to Random Forests as it also uses several Decision Trees, each tree using a random subset of features and observations. XGBoost can use regressor trees or classifier trees as well. But we will only focus on using it for Classification. Below we have an outline of the main differences between Random Forests and XGBoost Trees.\n",
        "\n",
        "|<font size=4>Random Forest|<font size=4>XGBoost Tree|\n",
        "|:---|:---|\n",
        "|<font size=3>All trees are created as a group and are independent|<font size=3>Trees are created sequentially (one tree at a time) <br> and depend on the previous trees residual outputs|\n",
        "|<font size=3>Uses **bagging**: *Boostrapping* + *Aggregation* <br> - *Boostrapping:* random sampling with replacement<br> - *Aggregation:* adding all trees outputs and majority <br> votes decides final forecast|<font size=3>Uses **boosting** each subsequent tree attempts to <br> create a better predictor than the preceding one, <br>random sampling of observations happen without <br>replacement|\n",
        "|<font size=3>Individual trees predict target labels|<font size=3>Individual trees predict **residuals**, which is the difference  <br> between the predicted and observed prediction values|\n",
        "\n",
        "We will later detail how residuals are calculated as we get further into the notebook.\n",
        "\n",
        "## **XGBoost General Structure**\n",
        "\n",
        "![xgboost.jpg](https://drive.google.com/uc?export=view&id=12nVLJtdBGBJUp2EMFcoXPBg8c24ewFSg)\n",
        "\n",
        "Each classifier tree in the XGBoost Model is built differently than the ones for Random Forest, we will see in more detail the different parts of an XGBoost tree later on."
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n",
        "\n",
        "Something to note if you decide to check the source documentation is that the import for XGBoost Classifier model uses a different package. Unlike all our previous models, that use \"sklearn\", this model comes in a separate package named \"xgboost\"."
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import packages for Gradient-Boosted Tree model\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_tree\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "\n",
        "Similar to the previous notebook we loaded the dataframe created in the first notebook of this tutorial, then we create a dataframe for each antibiotic. To check that our function works we will test it using the \"AMP\" antibiotic."
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_dfs.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training (features and labels) and Testing (features and labels)**\n",
        "\n",
        "Similarly to the previous notebook, we split our dataframe into training and testing chunks, each chunk with features and labels. Then we store these 4 sections into a a dictionary. We will then implement this function on the dataframe created in part 2."
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "  df_list = [All_Drugs_df[[\"Isolate\",drug]], All_Drugs_df.iloc[:,13:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "iP4HEW4qBO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test() for AMP example\n",
        "AMP_Train_test_dic = Split_train_test(\"AMP\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug AMP\n",
        "print(\"AMP\")\n",
        "for k, df in AMP_Train_test_dic.items():\n",
        "  print(k, df.shape)\n",
        "  # counting how many of the labels have susceptible versus resistant ones\n",
        "  if k.startswith(\"labels\"):\n",
        "    print(df.value_counts())"
      ],
      "metadata": {
        "id": "mIFeC9JvsOgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we have more Resistant E.Coli samples than Susceptible ones, unlike in our previous notebook. We will be looking at the AMP antibiotic and **the total number of training observations is 563.**"
      ],
      "metadata": {
        "id": "QjGPkIVPXx95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "**NOTE:** Same as prior notebook code"
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['G', 'S', 'GY', 'SY', 'GS', 'GYS']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # creating Year column filters for features_df\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Population structure column filters for features_df\n",
        "  pop_str_filter = [col for col in features_df if col.startswith(\"cutoff\")]\n",
        "  pop_struc_feat = features_df[pop_str_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in pop_str_filter and col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'S':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat]\n",
        "    S_feat_df = pd.concat(df_list, axis=1)\n",
        "    S_feat_df = S_feat_df.drop(columns=['Isolate'])\n",
        "    return S_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df\n",
        "\n",
        "  if combo == 'SY':\n",
        "    df_list = [features_df['Isolate'], pop_struc_feat, year_feat]\n",
        "    SY_feat_df = pd.concat(df_list, axis=1)\n",
        "    SY_feat_df = SY_feat_df.drop(columns=['Isolate'])\n",
        "    return SY_feat_df\n",
        "\n",
        "  if combo == 'GS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, pop_struc_feat]\n",
        "    GS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GS_feat_df = GS_feat_df.drop(columns=['Isolate'])\n",
        "    return GS_feat_df\n",
        "\n",
        "  if combo == 'GYS':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat, year_feat, pop_struc_feat, ]\n",
        "    GYS_feat_df = pd.concat(df_list, axis=1)\n",
        "    GYS_feat_df = GYS_feat_df.drop(columns=['Isolate'])\n",
        "    return GYS_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "AMP_SY_train_df = combo_feat(AMP_Train_test_dic['features_train'],\"AMP\",\"SY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"SY\" for drug \"AMP\" for training data\n",
        "AMP_SY_train_df.columns"
      ],
      "metadata": {
        "id": "iHsrRKadsRVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice here that for our example this time, we have picked only the features SY, meaning that when random sampling happens for the features, the columns from the Gene Absence Presence Dataset (**G**) will not be able to get picked."
      ],
      "metadata": {
        "id": "WTJmvTZbW3f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Gradient Boosted Trees model and training it per feature combination**\n",
        "\n",
        "Notice that in the creation of an Extreme Gradient Boosted Classifier model within our function, the training labels need to be encoded, that is be transformed from **R** to **0** and **S** to **1**. Our function, will also print the number of trees created."
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Gradient-Boosted Trees model function\n",
        "def run_GB(feat_train_df, lab_train, drug, combo):\n",
        "  # creating encoder to transform R and S into 0 and 1 respectively\n",
        "  labels = lab_train\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(labels)\n",
        "  labels_t = le.transform(labels)\n",
        "  # Creating the XGB Classifier model\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  GB =  XGBClassifier(random_state = 42)\n",
        "  # Training XGB Classifier model\n",
        "  GB = GB.fit(feat_train_df, labels_t)\n",
        "  # Checking number of trees in the model\n",
        "  print(\"Number of Decicion Trees in XGB Classifier model:\", GB.n_estimators)\n",
        "  return GB"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_GB() for specific drug feature combination dataframe\n",
        "GB_AMP_SY_model = run_GB(AMP_SY_train_df, AMP_Train_test_dic['labels_train'],\"AMP\",\"SY\")\n",
        "GB_AMP_SY_model"
      ],
      "metadata": {
        "id": "9H7HmGrWsUXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see there are many parameters we can tune in our XGB classifier. XGBoost is a preferred model by many practitioners because the model considers tuning how the decisions tree are created and trained (e.g. learning_rate and max_leaves). In addition, the model considers computer performance optimization (e.g. n_jobs) when calculating trees. If you want to tune your model feel free to read the [documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) for each of them. For now we will just use all default settings."
      ],
      "metadata": {
        "id": "yGblscmxbnOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost Classification Tree**\n",
        "\n",
        "Unlike Random Forests, we notice that the XGBoost is made of smaller looking trees, in this case we have a tree with a 6 level depth. You can change and look at the other trees by changing the argument **num_trees**. Also note that one of the main differences is that these trees are not predicting labels, instead they predict residuals. In the picture below, we can focus on just the bottom part of the XGBoost Classifier tree and we will pretend that there are 4 residuals in the last intermediate node:\n",
        "\n",
        "Let's first take a look at the first XGBoost Tree created:"
      ],
      "metadata": {
        "id": "hXUdlj_cdLwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the first XGBoost Tree\n",
        "plot_tree(GB_AMP_SY_model, num_trees=0)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(40, 15)"
      ],
      "metadata": {
        "id": "ZnkLhqlMgzgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that unlike the Random Forest classifier trees, XGboost trees have by default only 6 levels (counting from below the root node). But To get a better idea of the differences between xgboost trees and the ones from our prior module, we can focus on just one of the sections output below, the one that starts with the Decision threshold <font face=\"monospace\">Cutoff_92 $\\leq$ 117.5</font>\n",
        "\n",
        "![XGBoost-branch.jpg](https://drive.google.com/uc?export=view&id=1ODU_RvhreN2kLCX28UXQckKlMpBxtXwd)\n",
        "***\n",
        "**Feature Threshold** (Example: <font face=\"monospace\">Cutoff_92 $\\leq$ 117.5</font>)\n",
        "\n",
        " This is the same threshold explained in our previous Random Forest Notebook, if the boolean statement is TRUE or \"Yes\" then it goes to the left leaf, if FALSE or \"No\" then it goes to the right leaf. Notice that the arrows pointing towards left splits say (**Yes, missing**) This is because XGBoost Trees are also made to include observations that may have missing data for the feature being split on.\n",
        "***\n",
        "**Residuals** (Example: <font face=\"monospace\">[r1,r5,r3,r7]</font>)\n",
        "\n",
        "Eventhough these are not displayed in the original graphed tree from the code, we can imagine how these residuals are classified so we have an idea of how the algorithm works. For the sake of simplifying the explanation, let's imagine that the node with Decision threshold <font face=\"monospace\">Cutoff_92 $\\leq$ 117.5</font> has 4 residuals, r1 is the residual for observation 1, r5 for observation 5, so on and so forth. In each of the nodes of an xgboost tree, the residuals for each observation are being classified as we descend on the tree, we have r1 and r5 in the left leaf and r3 and r7 in the right leaf.\n",
        "***\n",
        "**Leaf** (Example: <font face=\"monospace\">Leaf=-0.150000006</font>)\n",
        "\n",
        "For XGboost trees, the leaves do not output the number of observations in each class, instead it displays a decimal, which is known as the **Output value**. This output is computed using only the residuals from the final leaf nodes, which in our pseudoexample would be r1 and r5.  Below its the actual formula to calculate it:\n",
        "\n",
        "$$Output =\\frac{\\sum Residuals}{\\sum[(Previous \\ Probability)*(1-Previous \\ Probability)] + \\lambda}$$\n",
        "\n",
        "- *$Residuals$* = The diference between the observed (0 for Resistance and 1 for Susceptibility) and predicted probabilities.\n",
        "\n",
        "- *$Previous \\ Probability$* = These are the probabilities predicted using the prior tree. Since this is an example from the first tree created, the default value for it is 0.5. This number will change as we keep building other trees.\n",
        "\n",
        "- *$\\lambda$*= Is the regularization term, the higher the number the lower the output value. This helps us to not follow too closely any particular observation so that our model generalizes well to new data (bias-variance trade-off). The default is set to 1. There are other regularization coeficcients, in this example *$\\lambda$* is equivalent to L2 regularization or Rigde Regression. Here is a [link](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0#:~:text=Therefore%2C%20the%20L1%20regularization%20decreases,lambda%20term%20increases%20the%20denominator.) for further mathematical explanation if interested.\n",
        "***\n",
        "\n",
        "Behind the simple looking XGboost tree there are different metrics used to decide each of the nodes and branches that shape the classifier tree. You can take a peak under the hood of this XGboost tree with the code below and check all the **Gain** and **Cover** scores at each split from the tree we graphed. We will briefly go over these concepts:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nH_v6rzWo-jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the metrics for each node of the XGboost classifier Trees\n",
        "print(GB_AMP_SY_model.get_booster().get_dump(with_stats = True)[0])"
      ],
      "metadata": {
        "id": "pwXpvbJBvOuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **OUTPUT INTERPRETATION of get_dump() for one xgboost Tree**\n",
        "To interpret each **(root and intermediate)** nodes output, we can take look at the following structure:\n",
        "\n",
        "<font color=\"gray\">`Parent_node #:[feature threshhold] yes=left_child_#, no=right_child_#, missing=left_child_#, gain=gain, cover=cover`\n",
        "\n",
        "To interpret each **leaf** nodes output, we can examine the below structure:\n",
        "\n",
        "<font color=\"gray\">`leaf_node_#:leaf=output_value, cover=cover`\n",
        "***\n",
        "For each node in our tree, we calculate the following:\n",
        "\n",
        "- **Similarity Scores (SS):** The scores indicate how similar are a particular grouping of residuals. This score is important because similar residuals imply similarity in observations, which helps with their classification. Below we can see the formula for SS:\n",
        "\n",
        "$$\\frac{(\\sum Residuals)^2}{\\sum[(Previous \\ Probability)*(1-Previous \\ Probability)] + \\lambda}$$\n",
        "\n",
        "<a name=\"gain-def\"></a>\n",
        "- **Gain:** This is calculated for each of the intermediate nodes and measures the relative contribution of the corresponding feature to the model. The higher this value the better the feature is at making a prediction. Below is the formula:\n",
        "\n",
        "$$Gain = SS_{(Left)} + SS_{(Right)} - SS_{(Parent \\ Node)}$$\n",
        "\n",
        "- **Cover:** is a metric to measure the number of observations affected by the split. This is essentially if the first part of the denominator in the similarity score:\n",
        "\n",
        "$$Cover = \\sum[(Previous \\ Probability)*(1-Previous \\ Probability)]$$\n",
        "***\n",
        "**NOTE:** When researching this model it's important to know that XGBoost $\\neq$ Gradient Boosted Tree. The Reason it has the **Extreme** denomination is that XGBoost builds on Gradient Boosted Tree in that it add different methods of Regularization (to help with generalization of the model and combat overfitting) and parallel processing (increase computing efficiency).\n"
      ],
      "metadata": {
        "id": "s02Uuck7GIW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Gradient Boosted Trees model**\n",
        "\n",
        "**XGBoost Final Decision Made**\n",
        "\n",
        "To understand how the final decision is made by the entire XGBoost model, we need to revisit again the equations we learned on the very first ML model we used **Logistic Regression**. Because XGBoost Classifier models uses them as well!\n",
        "\n",
        "**Logit Equation:** converts probabilities into log-odds predictions.\n",
        "$$ ln(\\frac{P}{1-P}) = \\hat \\beta_0 + \\hat \\beta_jX$$\n",
        "\n",
        "**Logistic Regression Equation:** converts log-odds predictions into probability predictions.\n",
        "$$P = \\frac{e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}{1+e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}$$\n",
        "\n",
        "Turns out these Equations are the ones that enables the XGboost Classifier to convert the predictions made by each of these XGboost Classifier trees into probabilities that will ultimately help us classify our observations into **Resistant (R)** or **Susceptible (S)**. Below we can see a graph of how it works:\n",
        "\n",
        "![final_pred.jpg](https://drive.google.com/uc?export=view&id=1RbLJnuLNpKsNMCF2gv9YLWZYYaXcmPwQ)\n",
        "\n",
        "Iteration 1:\n",
        "**Predicted log (odds1) = Initial Log(odds) + LR * Leaf Output that observation falls into tree 1**\n",
        "\n",
        "Iteration 2:\n",
        "**Predicted log (odds2) = Predicted Log(odds1) + LR * Leaf Output that observation falls into tree 2**\n",
        "\n",
        "ETC...\n",
        "\n",
        "At the end after we iterate to all xgboost trees (in this case 100 times), at each iteration a new tree is added and each time the predicted log (odds) gets updated with a better aproximation until we get a final **Predicted log (odds100)**, which takes in consideration a total of 100 trees. This final prediction is converted to a probability using the logit and logistic regression equations. This should give us the final prediction of **Resistant (R) if probability < 0.5** or **Susceptible (S) if probability > 0.5**\n",
        "\n",
        "**NOTE:** The Learning Rate in XGBoost is also called **\"eta\"** in other sources.\n",
        "***\n",
        "\n",
        "Below we can run a short piece of code to check the improvements for every 10 iterations out of a total of 100 to test how an XGBoost model works, we are not actually using our own model, the code creates automatically a training and validation sets, from the training data:"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a copy of the labels\n",
        "num_labels = AMP_Train_test_dic['labels_train'].copy()\n",
        "\n",
        "# transforms all labels of S into 1 and all labels of R into 0\n",
        "num_labels.loc[num_labels == \"S\"] = 1.0\n",
        "num_labels.loc[num_labels == \"R\"] = 0.0\n",
        "\n",
        "# create a matrix using the only the training data and the training labels coded as numbers\n",
        "d = xgb.DMatrix(AMP_SY_train_df, label=num_labels)\n",
        "\n",
        "# set up basic parameters that are similar to our model\n",
        "p = {'max_depth':6, 'eta':0.33, 'objective':'binary:logistic'}\n",
        "\n",
        "# prints results for every 10 iterations out of 100\n",
        "res = xgb.cv(params = p, dtrain=d, num_boost_round=100, verbose_eval=10, metrics=\"error\")"
      ],
      "metadata": {
        "id": "UPGNb3tTWb8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above shows us that the overall trend is that the error rate decreases as number of iteration increases. The outputs are composed of 2 numbers in the following format:\n",
        "\n",
        "`train-error: #1 + #2     test-error: #1 + #2`\n",
        "\n",
        "- **#1** is the error shown for training and testing sets, it is essentially (1- accuracy) for each 10th iteration.\n",
        "- **#2** is the standard deviation of the error rates.\n",
        "\n",
        "\n",
        "Now that we have an idea of how the XGBoost tree trains and makes its final predictions using all the trees. We can now make predictions with it, below a simple function is created similar to the code in the previous notebook:"
      ],
      "metadata": {
        "id": "EZCKtwd9kkkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "def predict(GB_combo_Model, features_test):\n",
        "  labels_pred = GB_combo_Model.predict(features_test)\n",
        "  return labels_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "AMP_SY_test_df = combo_feat(AMP_Train_test_dic['features_test'],\"AMP\",\"SY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"SY\" for drug \"AMP\" for testing data\n",
        "AMP_SY_test_df.columns"
      ],
      "metadata": {
        "id": "3Nj98S-BsblR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the predict() function using the feature combination \"GS\"\n",
        "AMP_SY_labels_pred = predict(GB_AMP_SY_model,AMP_SY_test_df)\n",
        "\n",
        "# transforming back our labels for interpretation in the next output\n",
        "labels_pred = np.where(AMP_SY_labels_pred<1,\"R\",\"S\")\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(labels_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "jquWRTCqsX50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "\n",
        "Similarly to the previous notebook, we will evaluate our XGBoost model by using a Confusion Matrix and respective metrics. Below is a quick review of these, remember that there is one Accuracy score, but Recall and Presicion should have as many sets as classes our model its trained to predict:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|\n",
        "|--|:-:|:-:\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook\n"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(GB_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  labels = labels_test\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(labels)\n",
        "  labels_t = le.transform(labels)\n",
        "  report = classification_report(labels_t, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    labels_pred = np.where(labels_pred<1,\"R\",\"S\")\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=np.where(GB_combo_model.classes_<1,\"R\",\"S\"))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.where(GB_combo_model.classes_<1,\"R\",\"S\"))\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['0']['recall'])\n",
        "    print('S recall:',report['1']['recall'])\n",
        "    print('R precision:',report['0']['precision'])\n",
        "    print('S precision:',report['1']['precision'])\n",
        "  return [report['accuracy'], report['0']['recall'], report['1']['recall'], report['0']['precision'], report['1']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(GB_AMP_SY_model, AMP_Train_test_dic['labels_test'],AMP_SY_labels_pred)"
      ],
      "metadata": {
        "id": "UGJIGTcNsgeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Create a feature importance graph showing which features were the most important**\n",
        "\n",
        "There are actually different ways to evaluate the importance of features in this model, some are based on the **Cover**, others in **Gain**, etc. Since we are already familiar with how these are calculated in [Part 5](#gain-def).\n",
        "\n",
        "The plot below will focus on the **Gain**. Since every node in every tree of our XGBoost model have their own gain, the value shown is just the average of each node. The Gain is the most relevant attribute to interpret the relative importance of each feature."
      ],
      "metadata": {
        "id": "os7345fZRPJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting feature importance (\"gain\")\n",
        "feat_imp = GB_AMP_SY_model.get_booster().get_score(importance_type=\"gain\")\n",
        "for k in feat_imp.keys():\n",
        "    feat_imp[k] = round(feat_imp[k],2)\n",
        "\n",
        "plot_importance(feat_imp, max_num_features = 20, importance_type = \"gain\", show_values=True, height=0.7)\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/XGBoost_AMP_SY_feat_importance.jpg',dpi=400, bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "H5Z-7_SlRwQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see that the most important feature in the model **GB_AMP_SY_model**, based in **Gain** metric is `cutoff_11059`.\n",
        "The Gain implies the relative contribution of the feature to the model. It takes each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction."
      ],
      "metadata": {
        "id": "uqjCgb-FdQjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) Use all functions and evaluate every drug in every feature combination!**\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "drug_list = All_Drugs_df.iloc[:,1:13].columns\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**"
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use all our functions this time and save our report into a single data structure\n",
        "GB_model_metrics = {}\n",
        "\n",
        "for drug in drug_list:\n",
        "  print(drug)\n",
        "  Test_Train_dic = Split_train_test(drug)\n",
        "  for combo in combo_list:\n",
        "    # Training each drug_combo features\n",
        "    labels_train = Test_Train_dic[\"labels_train\"]\n",
        "    features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "    GB_combo_model = run_GB(features_train, labels_train, drug, combo)\n",
        "\n",
        "    # Predicting each drug_combo features\n",
        "    features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "    labels_pred = predict(GB_combo_model, features_test)\n",
        "\n",
        "    # Evaluating our models\n",
        "    labels_test = Test_Train_dic[\"labels_test\"]\n",
        "    report = evaluate(GB_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "    GB_model_metrics[drug+\"_\"+combo] = report\n",
        "\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "glSAeIj2DrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **c) Store the metrics report for all drugs and features combinations as a csv file**"
      ],
      "metadata": {
        "id": "m8CQMKARZ1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dictionary into a dataframe\n",
        "GB_metrics = pd.DataFrame.from_dict(GB_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "GB_metrics = GB_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "# saving our metric results into a CSV file\n",
        "GB_metrics.to_csv(filepath+\"GB_metrics_df.csv\", index= False)\n",
        "GB_metrics\n"
      ],
      "metadata": {
        "id": "VmNdWS65fhRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have finally created and saved all the XGBoost Classifier results for every antibiotic and combination of features we were interested in. Next we will be moving away from tree-based or ensemble methods and learn about our last Machine Learning Method: [Neural Networks](https://colab.research.google.com/drive/1uxBtrr3F4r35ov9-AUswQS-WIWAO3TsU?usp=sharing)."
      ],
      "metadata": {
        "id": "KJVozj97pWvR"
      }
    }
  ]
}